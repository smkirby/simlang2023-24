{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulating Language, Lab 5, Rational Speech Act model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This lab introduces the Rational Speech Act model, which is a way of doing Bayesian modelling of pragmatic inference and rational behaviour during communication. The RSA model is potentially very general, but here we are going to use it to model pragmatic word use, and see how pragmatic speakers and listeners can go beyond literal word use to facilitate communication. Our model of word meaning builds on the model we used in Lab 2; now we are going to move beyond single words to (very small!) lexicons, and model inference during communication rather than learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Representing meanings, words, and lexicons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As in Lab 2, we are going to assume that the meaning of a word is the set of things that that word can be used to refer to, and we are going to model \"things that words refer to\" (referents) as numbers. So a word meaning is just a set of numbers, representing the set of things that the word can be used to refer to. As in Lab 2, we will represent word meanings as sets, e.g. like this:\n",
    "```python\n",
    "a_word_meaning = {0,1,2}\n",
    "```\n",
    "We also need a representation of words, which we will just represent as strings (in Python, you can represent these using single- or double-quoted strings of characters, e.g.:\n",
    "```python\n",
    "a_word = 'word'\n",
    "another_word = \"floccinaucinihilipilification\"\n",
    "```\n",
    "Now we have our representation of meanings and words, we can model a lexical entry, which is a pairing of a meaning and its associated word. We are going to represent these as *tuples* in Python, which for our purposes work a bit like lists but use round brackets instead of square brackets. So this is how we would represent a lexical entry for a word which refers to referents 0, 1 and 2:\n",
    "```python\n",
    "a_lexical_entry = ({0,1,2},'dax')\n",
    "```\n",
    "You can access the elements in a tuple by index in the same way you would pull items out from a list, e.g. `a_lexical_entry[0]` will give you the meaning from `a_lexical_entry`, `a_lexical_entry[1]` will pull out the word.\n",
    "\n",
    "Check you can access the meaning and word from `a_lexical_entry` in this way, then create your own lexical entry for a word, *fep*, which can be used to refer to referents 6, 7, 8, and 9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0, 1, 2}\n",
      "dax\n",
      "({8, 9, 6, 7}, 'fep')\n"
     ]
    }
   ],
   "source": [
    "a_lexical_entry = ({0,1,2},'dax')\n",
    "print(a_lexical_entry[0])\n",
    "print(a_lexical_entry[1])\n",
    "fep_lexical_entry = ({6,7,8,9},'fep')\n",
    "print(fep_lexical_entry)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a way of representing meanings, words, and single lexical entries, we can represent a lexicon, which is just going to be a list of lexical entries. For instance, a lexicon with three very specific words which refer to single referents, would be represented like this:\n",
    "```python\n",
    "lab_tutor_lexicon = [({0},'claire'),({1},'aislinn'),({2},'ponrawee')]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create `lab_tutor_lexicon` by copying that line of code into a cell. How would you access the first lexical entry from this lexicon? How would you access the meaning of the 2nd entry? How would you access the word from the 3rd entry? How would you count how many lexical entries the lexicon contains?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({0}, 'claire')\n",
      "{1}\n",
      "ponrawee\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "lab_tutor_lexicon = [({0},'claire'),({1},'aislinn'),({2},'ponrawee')]\n",
    "#first entry\n",
    "print(lab_tutor_lexicon[0])\n",
    "#meaning of second entry\n",
    "print(lab_tutor_lexicon[1][0])\n",
    "#word from third entry\n",
    "print(lab_tutor_lexicon[2][1])\n",
    "#number of lexical entries\n",
    "print(len(lab_tutor_lexicon))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The code\n",
    "\n",
    "Now we've introduced the notation we are going to be using to represent lexicons we can get on and build the model. As usual we start by loading the libraries we need and various libraries for manipulating log probabilities.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import logsumexp\n",
    "from math import log, log1p, exp\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions for dealing with log probabilities\n",
    "\n",
    "As per Lab 4, we are going to deal with log-probabilities rather than raw probabilities, so we need a couple of functions for dealing with those. I am also providing a little utility function that will convert log probabilities to normal probabiltities, which you might find helpful when it comes to looking at the model results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_subtract(x,y):\n",
    "    \"\"\"Takes two log numbers; returns their difference.\"\"\"\n",
    "    return x + log1p(-exp(y - x))\n",
    "\n",
    "def normalize_logprobs(logprobs):\n",
    "    \"\"\"Takes a list of log numbers; returns a list of scaled versions of those numbers that, \n",
    "    once converted to probabilities, sum to 1.\"\"\"\n",
    "    logtotal = logsumexp(logprobs) #calculates the summed log probabilities\n",
    "    normedlogs = []\n",
    "    for logp in logprobs:\n",
    "        normedlogs.append(logp - logtotal) #normalise - subtracting in the log domain\n",
    "                                           #is equivalent to dividing in the normal domain\n",
    "    return normedlogs\n",
    "\n",
    "def logprobs_to_probs(logprobs):\n",
    "    \"\"\"Takes a list of log numbers; converts them and returns a list of probabilities.\"\"\"\n",
    "    probs = []\n",
    "    for logp in logprobs:\n",
    "        probs.append(exp(logp))\n",
    "    return probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Possible words and possible referents\n",
    "We'll start out with a very simple two-word lexicon, where one of the words has a specific meaning and the other has a more general meaning that subsumes the meaning of the first word – that's `small_lexicon` below. \n",
    "\n",
    "In order to make the code work smoothly we also have to specify what words our model has to consider, and what possible referents there are that words can refer to. These are just lists. **Important point to note for when you are creating your own lexicons**: if your lexicon refers to a word that isn't in `possible_words` or referent that isn't in `possible_referents`, or there are possible words or referents that aren't covered in your lexicon, your code will break, (as in it will do unexpected things)! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_lexicon = [({0}, 'word1'), ({0, 1, 2}, 'word2')]\n",
    "possible_words = ['word1','word2']\n",
    "possible_referents = [0,1,2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A literal speaker and a literal listener"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll start by writing functions for our literal speaker and a literal listener. The literal speaker and literal listener just use words as dictated by their lexicon, without doing any fancy pragmatics. \n",
    "\n",
    "For our model of a literal speaker, we are going to provide a target referent (the object the literal speaker wants to refer to) and the speaker's lexicon, and the literal speaker will return a (log) probability distribution over possible words. The values in this probability distribution give the (log) probability of the literal speaker producing each of those words in order to label the target referent. The literal speaker behaves as you would expect – they are equally likely to use any of the words that include the target referent in their meaning. We also include a small probability of the literal speaker making a mistake and using the wrong word to refer to an object – this is specified by `error_probability`. It ensures we never deal with zero probabilities (which don't play nice in the log domain) but you can think of it as the probability of lexical access errors by the speaker. It can be very small if you like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_probability = 0.001 #you can make this as small as you like, as long as it's not 0\n",
    "\n",
    "def literal_speaker(target_referent,lexicon,possible_words,error_probability):\n",
    "    \"\"\"Takes speaker's target_referent (a single number); speaker's lexicon (list of tuples);\n",
    "        a list of possible words, and the probability of error.\n",
    "       Returns log probability distribution over all words in the lexicon\n",
    "       \"\"\"\n",
    "    word_probs = []\n",
    "    for candidate_word in possible_words: #consider each possible word\n",
    "        for meaning,word in lexicon: #work through the lexicon\n",
    "            if word==candidate_word: #if the word for this lexical entry is the one we want\n",
    "                if target_referent in meaning: #can this word be used to refer to this referent?\n",
    "                    word_probs.append(log(1-error_probability)) #if yes, it is likely to be used                \n",
    "                else:\n",
    "                    word_probs.append(log(error_probability)) #otherwise it is unlikely to be used \n",
    "    return normalize_logprobs(word_probs) #normalise these so they are true log probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that the literal speaker behaves as expected using `small_lexicon`. How likely are they to produce word1 to convey target referent 0? How likely are they to use word2? You can find this out by calling `literal_speaker(0,small_lexicon,possible_words,error_probability)` or, if you'd rather see probabilities than log probabilities, `logprobs_to_probs(literal_speaker(0,small_lexicon,possible_words,error_probability))`. If you want to see the words printed out alongside their probabilities of being produced you can do something like:\n",
    "```python\n",
    "s0_probabilities = logprobs_to_probs(literal_speaker(0,small_lexicon,possible_words,error_probability))\n",
    "for i in range(len(possible_words)):\n",
    "    print(possible_words[i],s0_probabilities[i])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*OK, so using the suggested code above, here are the probabilities for the two available words (word1 and word2) for a literal speaker labelling referent 0. Referent 0 can be labelled using either word, so the literal speaker just decides that both are equally possible.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word1 0.5\n",
      "word2 0.5\n"
     ]
    }
   ],
   "source": [
    "s0_probabilities = logprobs_to_probs(literal_speaker(0,small_lexicon,possible_words,error_probability))\n",
    "for i in range(len(possible_words)):\n",
    "    print(possible_words[i],s0_probabilities[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the literal speaker's behaviour for referents 0, 1 and 2, and make sure you understand why the literal speaker behaves the way they do."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*And here are the outputs for refernts 1 and 2. According to `small_lexicon` these can only be labelled using word 2, so that's what the literal speaker does (note that, because of `error_probability`, a small probability is assigned to word1 in both cases).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For referent 1\n",
      "word1 0.0010000000000000002\n",
      "word2 0.9989999999999999\n",
      "For referent 2\n",
      "word1 0.0010000000000000002\n",
      "word2 0.9989999999999999\n"
     ]
    }
   ],
   "source": [
    "print(\"For referent 1\")\n",
    "s0_probabilities = logprobs_to_probs(literal_speaker(1,small_lexicon,possible_words,error_probability))\n",
    "for i in range(len(possible_words)):\n",
    "    print(possible_words[i],s0_probabilities[i])\n",
    "\n",
    "print(\"For referent 2\")\n",
    "s0_probabilities = logprobs_to_probs(literal_speaker(2,small_lexicon,possible_words,error_probability))\n",
    "for i in range(len(possible_words)):\n",
    "    print(possible_words[i],s0_probabilities[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can do the same for a literal listener, who receives a word and, based on their lexicon, returns a (log) probability distribution over referents, indicating how likely they think it is that the word they heard refers to each possible referent. The literal hearer just assumes that all referents included in the word's meaning are equally likely, and any others are very unlikely (but can occur with some small probability given by `error_probability`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def literal_listener(received_word,lexicon,possible_referents,error_probability):\n",
    "    \"\"\"Takes listener's received word (a single string); listener's lexicon (list of tuples);\n",
    "        a list of possible referents, and the error probability\n",
    "       Returns log probability distribution over all possible meanings in the lexicon.\n",
    "    \"\"\"\n",
    "    referent_probs = []\n",
    "    for candidate_referent in possible_referents: #consider each possible referent in turn\n",
    "        for meaning,word in lexicon: #consider each lexical entry\n",
    "            if word==received_word: #if this lexical entry matches the received word\n",
    "                if candidate_referent in meaning: #if the candidate referent is included in the word's meaning \n",
    "                    referent_probs.append(log(1-error_probability)) #then it's likely to be this referent they arer talking about\n",
    "                else:\n",
    "                    referent_probs.append(log(error_probability)) #otherwise it's quite unlikely\n",
    "    return normalize_logprobs(referent_probs) #normalise at the end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How likely does a literal speaker who uses `small_lexicon` think each referent is after hearing word1? Again, you can see this probability distribution by running something like the following:\n",
    "```python\n",
    "l0_probabilities = logprobs_to_probs(literal_listener('word1',small_lexicon,possible_referents,error_probability))\n",
    "for i in range(len(possible_referents)):\n",
    "    print('referent',possible_referents[i],l0_probabilities[i])\n",
    "```\n",
    "Also check out the literal listener estimates given word2, and make sure you understand them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Here are the literal listener probabilities over each referent after hearing word1 and word2. Word1 can only be used to refer to referent 0, so that's where the literal listener assigns the probability; word2 is 3-ways-ambiguous for the literal listener (it can refer to referents 0, 1 or 2), so the probabiity is split over those three interpretations.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After hearing word1\n",
      "referent 0 0.9980019980019978\n",
      "referent 1 0.0009990009990009992\n",
      "referent 2 0.0009990009990009992\n",
      "After hearing word2\n",
      "referent 0 0.3333333333333333\n",
      "referent 1 0.3333333333333333\n",
      "referent 2 0.3333333333333333\n"
     ]
    }
   ],
   "source": [
    "print(\"After hearing word1\")\n",
    "l0_probabilities = logprobs_to_probs(literal_listener('word1',small_lexicon,possible_referents,error_probability))\n",
    "for i in range(len(possible_referents)):\n",
    "    print('referent',possible_referents[i],l0_probabilities[i])\n",
    "print(\"After hearing word2\")\n",
    "l0_probabilities = logprobs_to_probs(literal_listener('word2',small_lexicon,possible_referents,error_probability))\n",
    "for i in range(len(possible_referents)):\n",
    "    print('referent',possible_referents[i],l0_probabilities[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A pragmatic speaker and a pragmatic listener"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can move on to define our pragmatic speaker, who reasons about a literal listener when selecting their word to convey an intended referent, and a pragmatic listener, who reasons about a pragmatic speaker when interpreting words. Because the pragmatic listener model builds on the pragmatic speaker model, we will build the pragmatic speaker model first.\n",
    "\n",
    "Just like the literal speaker, our pragmatic speaker has a target referent they'd like to convey, and a lexicon that they are going to use. But unlike the literal speaker, who just looks up the target referent in their lexicon in a fairly dumb way, the pragmatic speaker is going to be pragmatic: they will reason about how a literal listener might interpret the word they produce, and weight their production in favour of words that are more likely to be interpreted as conveying their intended referent. The crucial line in the pragmatic speaker definition below is `literal_listener_prob_of_target_referent = literal_listener(candidate_word,lexicon,possible_referents,error_probability)[target_referent]`: if I use this particular `candidate_word`, what's the probability that the literal listener will correctly conclude I was talking about `target_referent`? For the pragmatic speaker, this *probability of correct interpretation by a literal listener* is what determines the probability of producing a particular word - the pragmatic speaker is more likely to use words that have a better chance of being interpreted correctly by the literal listener."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pragmatic_speaker(target_referent,lexicon,possible_words,possible_referents,error_probability):\n",
    "    \"\"\"Takes speaker's target_referent (a number); speaker's lexicon (list of tuples);\n",
    "        possible words, possible referents, and the error probability.\n",
    "       Returns log probability distribution over all possible words in the lexicon,\n",
    "       based on how likely a literal listener is to understand target_referent given each word.\n",
    "       \"\"\"\n",
    "    word_probs = []\n",
    "    for candidate_word in possible_words: #consider each candidate word\n",
    "        literal_listener_prob_of_target_referent = literal_listener(candidate_word,\n",
    "                                                                    lexicon,\n",
    "                                                                    possible_referents,\n",
    "                                                                    error_probability)[target_referent] #how \n",
    "                                                            #likely is the literal listener  \n",
    "                                                            #to choose target_referent if they hear this word?\n",
    "        word_probs.append(literal_listener_prob_of_target_referent)#note that down\n",
    "    return normalize_logprobs(word_probs) #and normalise at the end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use `pragmatic_speaker(0,small_lexicon,possible_words,possible_referents,error_probability)` to figure out how likely the pragmatic speaker is to produce each word in order to convey referent 0 to a literal listener; e.g. you could do:\n",
    "```python\n",
    "pragmatic_speaker_probabilities = logprobs_to_probs(pragmatic_speaker(0,small_lexicon,\n",
    "                                                                        possible_words,\n",
    "                                                                        possible_referents,\n",
    "                                                                        error_probability))\n",
    "for i in range(len(possible_words)):\n",
    "    print(possible_words[i],pragmatic_speaker_probabilities[i])\n",
    "```\n",
    "Are these the same or different than the literal speaker probabilities for conveying the same target referent? Can you figure out why the pragmatic speaker behaves like this? It might help to look back at the literal listener's behaviour. Similarly, what does the pragmatic speaker do to convey referents 1 and 2? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Here are the s1 pragmatic speaker probabilities for each word, given that they want to convey referent 0 to a pragmatic listener. Recall that the literal speaker would be equally likely to use word1 and word2 to do this. However, the pragmatic speaker knows that the literal listener finds word2 ambiguous (see above - l0 will interpret word2 as meaning referent 0 or 1 or 2). As a result the pragmatic speaker prefers to use word1 rather than word2 - s1 has 3 times to probability of using word1 because it's 3 times more likely to be correctly interpreted as conveying referent 0 by the literal listener.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word1 0.7496248124062032\n",
      "word2 0.2503751875937969\n"
     ]
    }
   ],
   "source": [
    "pragmatic_speaker_probabilities = logprobs_to_probs(pragmatic_speaker(0,small_lexicon,\n",
    "                                                                        possible_words,\n",
    "                                                                        possible_referents,\n",
    "                                                                        error_probability))\n",
    "for i in range(len(possible_words)):\n",
    "    print(possible_words[i],pragmatic_speaker_probabilities[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Here's the same stuff for a pragmatic speaker attempting to convey referents 1 and 2 - there's nothing fancy going on here, a literal listener would never interpret word1 as conveying either of these referents so the pragmatic speaker has no choice but to use word2.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For referent 1\n",
      "word1 0.002988047808764942\n",
      "word2 0.9970119521912352\n",
      "For referent 2\n",
      "word1 0.002988047808764942\n",
      "word2 0.9970119521912352\n"
     ]
    }
   ],
   "source": [
    "print(\"For referent 1\")\n",
    "pragmatic_speaker_probabilities = logprobs_to_probs(pragmatic_speaker(1,small_lexicon, \n",
    "                                                                        possible_words,\n",
    "                                                                        possible_referents,\n",
    "                                                                        error_probability))\n",
    "for i in range(len(possible_words)):\n",
    "    print(possible_words[i],pragmatic_speaker_probabilities[i])\n",
    "print(\"For referent 2\")\n",
    "pragmatic_speaker_probabilities = logprobs_to_probs(pragmatic_speaker(2,small_lexicon, \n",
    "                                                                        possible_words,\n",
    "                                                                        possible_referents,\n",
    "                                                                        error_probability))\n",
    "for i in range(len(possible_words)):\n",
    "    print(possible_words[i],pragmatic_speaker_probabilities[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have the pragmatic speaker we can define our pragmatic listener, who reasons about the behaviour of a pragmatic speaker to calculate a probability distribution over referents given a received word. The key line in the definition of our pragmatic listener is `pragmatic_speaker_prob_of_word_given_meaning = pragmatic_speaker(candidate_referent,lexicon,possible_words,possible_referents,error_probability)[word_index]`: how likely is a pragmatic speaker to use this word to convey some `candidate_referent`? That determines the pragmatic listener's interpretation of the word – they reason about how a pragmatic speaker would be likely to behave, and interpret the word they received accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pragmatic_listener(received_word,lexicon,possible_words,possible_referents,error_probability):\n",
    "    \"\"\" \n",
    "    Takes listener's received word (a single string); listener's lexicon (list of tuples);\n",
    "    possible words, possible referents, and error probability.\n",
    "    Returns log probability distribution over all possible meanings in the lexicon,\n",
    "    based on how likely a pragmatic speaker is to use this word to refer to each possible meaning.\n",
    "    \"\"\"\n",
    "    word_index = possible_words.index(received_word) #.index tells me the index of received_word in possible_words\n",
    "    referent_probs = []\n",
    "    for candidate_referent in possible_referents: #consider each possible referent\n",
    "        pragmatic_speaker_prob_of_word_given_meaning = pragmatic_speaker(candidate_referent,\n",
    "                                                                         lexicon,\n",
    "                                                                         possible_words,\n",
    "                                                                         possible_referents,\n",
    "                                                                         error_probability)[word_index] #how likely \n",
    "                                                                #is the pragmatic speaker\n",
    "                                                                #to use this word to convey this candidate referent?\n",
    "        referent_probs.append(pragmatic_speaker_prob_of_word_given_meaning) #note that down\n",
    "    return normalize_logprobs(referent_probs) #and normalise at the end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does a pragmatic listener who uses `small_lexicon` interpret word 1? You can get a nicely formatted list as follows:\n",
    "```python\n",
    "pragmatic_listener_probabilities = logprobs_to_probs(pragmatic_listener('word1',small_lexicon,\n",
    "                                                                        possible_words,\n",
    "                                                                        possible_referents,\n",
    "                                                                        error_probability))\n",
    "for i in range(len(possible_referents)):\n",
    "    print('referent',possible_referents[i],pragmatic_listener_probabilities[i])\n",
    "```\n",
    "And how does the pragmatic listener interpret word2? Again, make sense of these behaviours in terms of the behaviour of the pragmatic speaker, who the pragmatic listener is reasoning about."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*OK, so here are the pragmatic listener's probabilities for each referent after hearing word1 and word2. Just like for the literal listener, word1 is unambiguous - it can only be used to label referent 0 by the pragmatic speaker (because it can only be interpreted as meaning referent 0 by the literal listener), so the pragmatic listener interprets it as conveying referent 0. Stuff gets more interesting for word2. Remember that for a literal listener this word was 3-ways ambiguous, equally likely to be interpreted as conveying referents 0, 1 or 2. However, a pragmatic speaker has a preferred way of conveying referent 0 - they will tend to use word1, and will mainly use word 2 to refer to referents 1 and 2. That means, for a pragmatic listener reasoning about the pragmatic speaker, word2 becomes less ambiguous - the pragmatic speaker is unlikely to use it to refer to referent 0, so the pragmatic listener tends to interpret it as conveying referent 1 or referent 2 (equally likely).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After hearing word1\n",
      "referent 0 0.9920909364267967\n",
      "referent 1 0.003954531786601677\n",
      "referent 2 0.003954531786601677\n",
      "After hearing word2\n",
      "referent 0 0.11155555555555559\n",
      "referent 1 0.4442222222222223\n",
      "referent 2 0.4442222222222223\n"
     ]
    }
   ],
   "source": [
    "print(\"After hearing word1\")\n",
    "pragmatic_listener_probabilities = logprobs_to_probs(pragmatic_listener('word1',small_lexicon,\n",
    "                                                                        possible_words,\n",
    "                                                                        possible_referents,\n",
    "                                                                        error_probability))\n",
    "for i in range(len(possible_referents)):\n",
    "    print('referent',possible_referents[i],pragmatic_listener_probabilities[i])\n",
    "print(\"After hearing word2\")\n",
    "pragmatic_listener_probabilities = logprobs_to_probs(pragmatic_listener('word2',small_lexicon,\n",
    "                                                                        possible_words,\n",
    "                                                                        possible_referents,\n",
    "                                                                        error_probability))\n",
    "for i in range(len(possible_referents)):\n",
    "    print('referent',possible_referents[i],pragmatic_listener_probabilities[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions\n",
    "\n",
    "The main aim for today's lab is to work through the questions above and make sure your understand why the RSA model behaves the way it does. Once you are happy with that, attempt the questions below.\n",
    "1. How do synonymy (several ways of expressing a given meaning) and ambiguity (several meanings expressed using the same signal) in the lexicon affect communication between literal speakers and literal listeners? How does the availability of pragmatic inference (i.e. moving from literal to pragmatic speakers and listeners) change the consequences of ambiguity? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*The `small_lexicon` example illustrates these various points. Ambiguity is generally a problem, and especially so for literal speakers and listeners. For example, imagine a literal speaker attempting to convey referent 1 to a literal listener. The literal speaker uses word2, because that's what's specified in their lexicon. However, the literal listener will often interpret word2 as conveying referent 0 or referent 2, because it's ambiguous; as a result, they will fail to communicate about referent 1 most of the time (on average, two thirds of the time), and the same for referent 2. They also don't do great with referent 0, but this time because of the combination of synonymy and ambiguity in `small_lexicon`. There are two ways of expressing referent 0, which in itself isn't necessarily a problem – you can construct a toy lexicon where every referent has several words associated with it and where perfect communication is possible. But in `small_lexicon` one of the synonymous forms, word2, is ambiguous; as a result, the literal speaker will often use word2 to label referent 0, and when that happens the literal listener is unlikely to arrive at the correct interpretation.*\n",
    "\n",
    "*Some but not all of these problems are reduced for pragmatic speakers and listeners. In particular, the pragmatic speaker is aware of the ambiguity of word2, and as a result tends to avoid it when other options are available (i.e. for referent 0, where word1 is a better option). For referents 1 and 2 they have no choice to but to use the highly ambiguous word2, because they have no other means available for expressing those referents. But the pragmatic listener can reduce the ambiguity a little, because they know that the pragmatic speaker has a better way of expressing referent 0, so word2 becomes effectively 2-and-a-bit-ways ambiguous. Note that it's not magic though – in particular, the problems posed by ambiguity are reduced, but not eliminated.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Are there cases where perfect communication between pragmatic individuals is possible even if their lexicon contains ambiguous entries? Is perfect communication possible when the lexicon *only* contains ambiguous entries? Play around with different lexicons to find out."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*The aim here was encourage you to play around with different lexicons to explore this. In answer to the first part of the question: I don't think **perfect** communication is possible when there's ambiguity, but decent communication should be possible even when the lexicon contains ambiguity. For example, in `small_lexicon2` below (a minor extension to `small_lexicon`), there is one highly ambiguous word but in practice, for pragmatic speakers and listeners, they still communicate succesfully most of the time.* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_lexicon2 = [({0}, 'word1'), ({0, 1, 2}, 'word2'),({2}, 'word3')]\n",
    "possible_words = ['word1','word2','word3']\n",
    "possible_referents = [0,1,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pragmatic speaker, for referent 0\n",
      "word1 0.7490627343164209\n",
      "word2 0.25018745313671586\n",
      "word3 0.0007498125468632841\n",
      "Pragmatic speaker, for referent 1\n",
      "word1 0.002979145978152929\n",
      "word2 0.9940417080436941\n",
      "word3 0.002979145978152929\n",
      "Pragmatic speaker, for referent 2\n",
      "word1 0.0007498125468632841\n",
      "word2 0.2501874531367158\n",
      "word3 0.7490627343164209\n"
     ]
    }
   ],
   "source": [
    "print(\"Pragmatic speaker, for referent 0\")\n",
    "pragmatic_speaker_probabilities = logprobs_to_probs(pragmatic_speaker(0,small_lexicon2,possible_words,possible_referents,error_probability))\n",
    "for i in range(len(possible_words)):\n",
    "    print(possible_words[i],pragmatic_speaker_probabilities[i])\n",
    "print(\"Pragmatic speaker, for referent 1\")\n",
    "pragmatic_speaker_probabilities = logprobs_to_probs(pragmatic_speaker(1,small_lexicon2,possible_words,possible_referents,error_probability))\n",
    "for i in range(len(possible_words)):\n",
    "    print(possible_words[i],pragmatic_speaker_probabilities[i])\n",
    "print(\"Pragmatic speaker, for referent 2\")\n",
    "pragmatic_speaker_probabilities = logprobs_to_probs(pragmatic_speaker(2,small_lexicon2,possible_words,possible_referents,error_probability))\n",
    "for i in range(len(possible_words)):\n",
    "    print(possible_words[i],pragmatic_speaker_probabilities[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pragmatic listener, after hearing word1\n",
      "referent 0 0.9950464935247344\n",
      "referent 1 0.003957463939204809\n",
      "referent 2 0.0009960425360607953\n",
      "Pragmatic listener, after hearing word2\n",
      "referent 0 0.1674147963424772\n",
      "referent 1 0.6651704073150456\n",
      "referent 2 0.16741479634247713\n",
      "Pragmatic listener, after hearing word3\n",
      "referent 0 0.0009960425360607953\n",
      "referent 1 0.003957463939204809\n",
      "referent 2 0.9950464935247344\n"
     ]
    }
   ],
   "source": [
    "print(\"Pragmatic listener, after hearing word1\")\n",
    "pragmatic_listener_probabilities = logprobs_to_probs(pragmatic_listener('word1',small_lexicon2,possible_words,possible_referents,error_probability))\n",
    "for i in range(len(possible_referents)):\n",
    "    print('referent',possible_referents[i],pragmatic_listener_probabilities[i])\n",
    "print(\"Pragmatic listener, after hearing word2\")\n",
    "pragmatic_listener_probabilities = logprobs_to_probs(pragmatic_listener('word2',small_lexicon2,possible_words,possible_referents,error_probability))\n",
    "for i in range(len(possible_referents)):\n",
    "    print('referent',possible_referents[i],pragmatic_listener_probabilities[i])\n",
    "print(\"Pragmatic listener, after hearing word3\")\n",
    "pragmatic_listener_probabilities = logprobs_to_probs(pragmatic_listener('word3',small_lexicon2,possible_words,possible_referents,error_probability))\n",
    "for i in range(len(possible_referents)):\n",
    "    print('referent',possible_referents[i],pragmatic_listener_probabilities[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Regarding the second part of the question, I couldn't come up with any cases where the lexicon contains **only** ambiguous cases but communication nonetheless succeeds most of the time – in the examples above, there is always at least one signal which is unambiguous, and reasoning about the use of that unambiguous signal reduces ambiguity elsewhere. So for instance, `lexicon3` should work OK despite using 3 ambiguous words, but it's reliant on word1 being unambiguous.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "lexicon3 = [({0}, 'word1'), ({0, 1, 2, 3}, 'word2'),({1,2,3}, 'word3'),({0,3}, 'word4')]\n",
    "possible_words = ['word1','word2','word3','word4']\n",
    "possible_referents = [0,1,2,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pragmatic speaker, for referent 0\n",
      "word1 0.5707484649084265\n",
      "word2 0.1431156060656265\n",
      "word3 0.00019094810682538586\n",
      "word4 0.2859449809191217\n",
      "Pragmatic speaker, for referent 1\n",
      "word1 0.0017068062190903897\n",
      "word2 0.4275549578821427\n",
      "word3 0.5698831259830027\n",
      "word4 0.0008551099157642853\n",
      "Pragmatic speaker, for referent 2\n",
      "word1 0.0017068062190903897\n",
      "word2 0.4275549578821427\n",
      "word3 0.5698831259830027\n",
      "word4 0.0008551099157642853\n",
      "Pragmatic listener, after hearing word1\n",
      "referent 0 0.9924627954592974\n",
      "referent 1 0.002967930315463129\n",
      "referent 2 0.002967930315463129\n",
      "referent 3 0.0016013439097762804\n",
      "Pragmatic listener, after hearing word2\n",
      "referent 0 0.11645712751754302\n",
      "referent 1 0.3479132962481121\n",
      "referent 2 0.3479132962481121\n",
      "referent 3 0.1877162799862328\n",
      "Pragmatic listener, after hearing word3\n",
      "referent 0 0.0001319215244314055\n",
      "referent 1 0.3937187541542961\n",
      "referent 2 0.3937187541542961\n",
      "referent 3 0.21243057016697636\n"
     ]
    }
   ],
   "source": [
    "print(\"Pragmatic speaker, for referent 0\")\n",
    "pragmatic_speaker_probabilities = logprobs_to_probs(pragmatic_speaker(0,lexicon3,possible_words,possible_referents,error_probability))\n",
    "for i in range(len(possible_words)):\n",
    "    print(possible_words[i],pragmatic_speaker_probabilities[i])\n",
    "print(\"Pragmatic speaker, for referent 1\")\n",
    "pragmatic_speaker_probabilities = logprobs_to_probs(pragmatic_speaker(1,lexicon3,possible_words,possible_referents,error_probability))\n",
    "for i in range(len(possible_words)):\n",
    "    print(possible_words[i],pragmatic_speaker_probabilities[i])\n",
    "print(\"Pragmatic speaker, for referent 2\")\n",
    "pragmatic_speaker_probabilities = logprobs_to_probs(pragmatic_speaker(2,lexicon3,possible_words,possible_referents,error_probability))\n",
    "for i in range(len(possible_words)):\n",
    "    print(possible_words[i],pragmatic_speaker_probabilities[i])\n",
    "print(\"Pragmatic listener, after hearing word1\")\n",
    "pragmatic_listener_probabilities = logprobs_to_probs(pragmatic_listener('word1',lexicon3,possible_words,possible_referents,error_probability))\n",
    "for i in range(len(possible_referents)):\n",
    "    print('referent',possible_referents[i],pragmatic_listener_probabilities[i])\n",
    "print(\"Pragmatic listener, after hearing word2\")\n",
    "pragmatic_listener_probabilities = logprobs_to_probs(pragmatic_listener('word2',lexicon3,possible_words,possible_referents,error_probability))\n",
    "for i in range(len(possible_referents)):\n",
    "    print('referent',possible_referents[i],pragmatic_listener_probabilities[i])\n",
    "print(\"Pragmatic listener, after hearing word3\")\n",
    "pragmatic_listener_probabilities = logprobs_to_probs(pragmatic_listener('word3',lexicon3,possible_words,possible_referents,error_probability))\n",
    "for i in range(len(possible_referents)):\n",
    "    print('referent',possible_referents[i],pragmatic_listener_probabilities[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. [Mainly a coding problem] There's no reason to stop at a pragmatic listener who reasons about a pragmatic speaker who reasons about a a literal listener - we could happy model a *level two* pragmatic speaker (a pragmatic speaker who reasons about a pragmatic listener), and a *level two* pragmatic listener (a pragmatic listener who reasons about a level two pragmatic speaker), or even higher levels of pragmatic recursion. Either adapt the code for `pragmatic_speaker` and `pragmatic_listener` to produce code for `level_two_pragmatic_speaker` and `level_two_pragmatic_listener` (which should be relatively straightforward) or (more ambitiously), write two recursive functions, `level_n_pragmatic_speaker` and `level_n_pragmatic_listener` which can handle *any* level of recursion. Using either of these techniques, what happens when we model higher and higher levels of pragmatic reasoning? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*I will just provide the non-recursive definitions for level two pragmatic speakers and listeners and leave you the fun of writing the recursive one yourself – the only thing to remember if you attempt that is that speaker n models listener n-1, but listener n models speaker n, that's just the naming convention. Notice that my speaker 2 and listener 2 definitions are literally just cut-and-paste of the speaker 1 and listener 1 definitions, but speaker 2 reasons about listener 1 rather than the literal listener (aka. listener 0) and listener 2 reasons about speaker 2 rather than speaker 1.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def level_two_pragmatic_speaker(target_referent,lexicon,possible_words,possible_referents,error_probability):\n",
    "    \"\"\"Takes speaker's target_referent (a number)l speaker's lexicon (list of tuples);\n",
    "        possible words, possible referents, and an error probability.\n",
    "       Returns log probability distribution over all possible words in the lexicon,\n",
    "       based on how likely a pragmatic listener is to understand target_referent given that word.\n",
    "       \"\"\"\n",
    "    word_probs = []\n",
    "    for candidate_word in possible_words: #consider each candidate word\n",
    "        pragmatic_listener_prob_of_target_referent = pragmatic_listener(candidate_word,\n",
    "                                                                    lexicon,\n",
    "                                                                    possible_words,\n",
    "                                                                    possible_referents,\n",
    "                                                                    error_probability)[target_referent] #how \n",
    "                                                            #likely is the pragmatic listener  \n",
    "                                                            #to choose target_referent if they hear this word?\n",
    "        word_probs.append(pragmatic_listener_prob_of_target_referent)#note that down\n",
    "    return normalize_logprobs(word_probs) #and normalise at the end\n",
    "\n",
    "def level_two_pragmatic_listener(received_word,lexicon,possible_words,possible_referents,error_probability):\n",
    "    \"\"\" \n",
    "    Takes listener's received word (a single string); listener's lexicon (list of tuples);\n",
    "    possible words, possible referents, and error probability.\n",
    "    Returns log probability distribution over all possible meanings in the lexicon,\n",
    "    based on how likely a level two pragmatic speaker is to use this word to refer to each possible meaning.\n",
    "    \"\"\"\n",
    "    word_index = possible_words.index(received_word) #.index tells me the index of received_word in possible_words\n",
    "    referent_probs = []\n",
    "    for candidate_referent in possible_referents: #consider each possible referent\n",
    "        level_two_pragmatic_speaker_prob_of_word_given_meaning = level_two_pragmatic_speaker(candidate_referent,\n",
    "                                                                         lexicon,\n",
    "                                                                         possible_words,\n",
    "                                                                         possible_referents,\n",
    "                                                                         error_probability)[word_index] #how likely \n",
    "                                                                #is the level two pragmatic speaker\n",
    "                                                                #to use this word to convey this candidate referent?\n",
    "        referent_probs.append(level_two_pragmatic_speaker_prob_of_word_given_meaning) #note that down\n",
    "    return normalize_logprobs(referent_probs) #and normalise at the end\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*To see what difference this makes, let's try these level two pragmatic speakers and listeners on `small_lexicon`, which we already tested extensively above. Remember I have to redefine `possible_words` and `possible_referents` whenever I redefine the lexicon.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_lexicon = [({0}, 'word1'), ({0, 1, 2}, 'word2')]\n",
    "possible_words = ['word1','word2']\n",
    "possible_referents = [0,1,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pragmatic speaker, for referent 0\n",
      "word1 0.7496248124062032\n",
      "word2 0.2503751875937969\n",
      "Pragmatic speaker, for referent 1\n",
      "word1 0.002988047808764942\n",
      "word2 0.9970119521912352\n",
      "Pragmatic speaker, for referent 2\n",
      "word1 0.002988047808764942\n",
      "word2 0.9970119521912352\n",
      "Level two pragmatic speaker, for referent 0\n",
      "word1 0.8989209349497581\n",
      "word2 0.10107906505024204\n",
      "Level two pragmatic speaker, for referent 1\n",
      "word1 0.00882359861645974\n",
      "word2 0.9911764013835402\n",
      "Level two pragmatic speaker, for referent 2\n",
      "word1 0.00882359861645974\n",
      "word2 0.9911764013835402\n"
     ]
    }
   ],
   "source": [
    "print(\"Pragmatic speaker, for referent 0\")\n",
    "pragmatic_speaker_probabilities = logprobs_to_probs(pragmatic_speaker(0,small_lexicon2,possible_words,possible_referents,error_probability))\n",
    "for i in range(len(possible_words)):\n",
    "    print(possible_words[i],pragmatic_speaker_probabilities[i])\n",
    "print(\"Pragmatic speaker, for referent 1\")\n",
    "pragmatic_speaker_probabilities = logprobs_to_probs(pragmatic_speaker(1,small_lexicon2,possible_words,possible_referents,error_probability))\n",
    "for i in range(len(possible_words)):\n",
    "    print(possible_words[i],pragmatic_speaker_probabilities[i])\n",
    "print(\"Pragmatic speaker, for referent 2\")\n",
    "pragmatic_speaker_probabilities = logprobs_to_probs(pragmatic_speaker(2,small_lexicon2,possible_words,possible_referents,error_probability))\n",
    "for i in range(len(possible_words)):\n",
    "    print(possible_words[i],pragmatic_speaker_probabilities[i])\n",
    "print(\"Level two pragmatic speaker, for referent 0\")\n",
    "level_two_pragmatic_speaker_probabilities = logprobs_to_probs(level_two_pragmatic_speaker(0,small_lexicon2,possible_words,possible_referents,error_probability))\n",
    "for i in range(len(possible_words)):\n",
    "    print(possible_words[i],level_two_pragmatic_speaker_probabilities[i])\n",
    "print(\"Level two pragmatic speaker, for referent 1\")\n",
    "level_two_pragmatic_speaker_probabilities = logprobs_to_probs(level_two_pragmatic_speaker(1,small_lexicon2,possible_words,possible_referents,error_probability))\n",
    "for i in range(len(possible_words)):\n",
    "    print(possible_words[i],level_two_pragmatic_speaker_probabilities[i])\n",
    "print(\"Level two pragmatic speaker, for referent 2\")\n",
    "level_two_pragmatic_speaker_probabilities = logprobs_to_probs(level_two_pragmatic_speaker(2,small_lexicon2,possible_words,possible_referents,error_probability))\n",
    "for i in range(len(possible_words)):\n",
    "    print(possible_words[i],level_two_pragmatic_speaker_probabilities[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*So the level two speaker just shows a stronger preference to avoid word2 when labelling referent 0, because they know that l1 is likely to interpret word2 as conveying something other than referent 0 despite its literal ambiguity.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pragmatic listener, after hearing word1\n",
      "referent 0 0.9920909364267967\n",
      "referent 1 0.003954531786601677\n",
      "referent 2 0.003954531786601677\n",
      "Pragmatic listener, after hearing word2\n",
      "referent 0 0.11155555555555559\n",
      "referent 1 0.4442222222222223\n",
      "referent 2 0.4442222222222223\n",
      "Level two pragmatic listener, after hearing word1\n",
      "referent 0 0.9807464425029756\n",
      "referent 1 0.009626778748512222\n",
      "referent 2 0.009626778748512222\n",
      "Level two pragmatic listener, after hearing word2\n",
      "referent 0 0.04851565660082567\n",
      "referent 1 0.47574217169958716\n",
      "referent 2 0.47574217169958716\n"
     ]
    }
   ],
   "source": [
    "print(\"Pragmatic listener, after hearing word1\")\n",
    "pragmatic_listener_probabilities = logprobs_to_probs(pragmatic_listener('word1',small_lexicon2,possible_words,possible_referents,error_probability))\n",
    "for i in range(len(possible_referents)):\n",
    "    print('referent',possible_referents[i],pragmatic_listener_probabilities[i])\n",
    "print(\"Pragmatic listener, after hearing word2\")\n",
    "pragmatic_listener_probabilities = logprobs_to_probs(pragmatic_listener('word2',small_lexicon2,possible_words,possible_referents,error_probability))\n",
    "for i in range(len(possible_referents)):\n",
    "    print('referent',possible_referents[i],pragmatic_listener_probabilities[i])\n",
    "print(\"Level two pragmatic listener, after hearing word1\")\n",
    "level_two_pragmatic_listener_probabilities = logprobs_to_probs(level_two_pragmatic_listener('word1',small_lexicon2,possible_words,possible_referents,error_probability))\n",
    "for i in range(len(possible_referents)):\n",
    "    print('referent',possible_referents[i],level_two_pragmatic_listener_probabilities[i])\n",
    "print(\"Level two pragmatic listener, after hearing word2\")\n",
    "level_two_pragmatic_listener_probabilities = logprobs_to_probs(level_two_pragmatic_listener('word2',small_lexicon2,possible_words,possible_referents,error_probability))\n",
    "for i in range(len(possible_referents)):\n",
    "    print('referent',possible_referents[i],level_two_pragmatic_listener_probabilities[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Similarly, the second level listener shows a slight sharpening of the patterns seen in the first level listener. The gains don't look like they'd be huge though, so assuming there is some cost to extra levels of recursive reasoning, you might wonder whether and when it's worth the cognitive effort.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. [Hard] Our `literal_listener` and `literal_speaker` functions are models of how a (literal or pragmatic) speaker would speak given a particular lexicon: in other words, it's modelling a likelihood, $p(data|lexicon)$, where data is a pairing of intended referents and words. Think about how you could use this as the basis for a model of word learning where the learner's task is to infer an entire lexicon based on the productions of a speaker using that lexicon (Hint: to do this you need to decide what the hypothesis space is and what the prior is). Is learning easier or harder when you learn from a literal or pragmatic speaker? If you are feeling brave, implement the model, otherwise have a think about what kinds of challenges a learner learning from a pragmatic speaker might face when it comes to inferring the \"correct\" (i.e. same as the teacher's) word meanings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*So in this model the hypothesis space would be the set of all possible lexicons (which might get very large!) and we'd have to define a prior over lexicons, which commits us to making a decision about which lexicons are more likely. You could just use a uniform prior, but we'll see in the next lab (when we model compositionality) that actually thinking about things like the complexity of the lexicon (how many distinct forms does it use?) makes sense in these cases.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*I haven't provided an implementation of this full model, but just thinking about it: one of the challenges facing a learner who learns from pragmatic speakers is that the way they use words doesn't reflect their **real** meaning, which might meake it hard for learners to figure out what that real meaning is. For instance, in the `small_lexicon` example above, both the speaker and listener thing that word2 refers to referents 0, 1 and 2, but in practice when they use the word they tend to avoid using it for referent 0. As a result, and based on a small sample of observations of this behaviour, a learner might learn the wrong meaning for word2, namely `{1,2}`. That's the wrong thing to do if your goal is to infer the same lexicon as the person generating your data, but it does suggest that, if combined with iterated learning, this might work nicely as a model of language change where initial pragmatic flexibility becomes lexicalised/grammaticalised, resulting in changes in the meaning of words or expressions.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "2254d95886abd510fdbe4740a73329c02480b002321aba449e3a8ac32ef99413"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
