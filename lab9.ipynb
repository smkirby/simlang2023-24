{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulating Language, Lab 9, Gene-culture co-evolution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to use the same code as the last lab to do something similar to Smith & Kirby (2008) and discover what types of prior and learning strategy combinations are evolutionarily stable. You may be surprised to find that we really don't need much more than the code we already have to do this!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code from Lab 8\n",
    "\n",
    "Here's the code from Lab 8, with no changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from math import log, log1p, exp\n",
    "from scipy.special import logsumexp\n",
    "\n",
    "from numpy import mean # This is a handy function that calculate the average of a list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup space of possible languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_languages(n_variables, n_variants):\n",
    "    \"\"\"Takes n_variables (number of variables in a language) and n_variants (number of variants of each variable);\n",
    "       returns list of all possible languages (all possible ways of combining the variants)\n",
    "       \"\"\"\n",
    "    if n_variables == 0:\n",
    "        return [[]] # The list of all languages with zero variables is just one language, and that's empty\n",
    "    else:\n",
    "        result = [] # If we are looking for a list of languages with more than zero variables, \n",
    "                    # then we'll need to build a list\n",
    "        smaller_langs = all_languages(n_variables - 1, n_variants) # Let's first find all the languages with one \n",
    "                                                               # fewer variables\n",
    "        for language in smaller_langs: # For each of these smaller languages, we're going to have to create a more\n",
    "                                       # complex language by adding each of the possible variants\n",
    "            for variant in range(n_variants):\n",
    "                result.append(language + [variant])\n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log probability functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_subtract(x,y):\n",
    "    \"\"\"Takes two log numbers; returns their difference.\"\"\"\n",
    "    return x + log1p(-exp(y - x))\n",
    "\n",
    "def normalize_logprobs(logprobs):\n",
    "    \"\"\"Takes a list of log numbers; returns a list of scaled versions of those numbers that, \n",
    "    once converted to probabilities, sum to 1.\"\"\"\n",
    "    logtotal = logsumexp(logprobs) #calculates the summed log probabilities\n",
    "    normedlogs = []\n",
    "    for logp in logprobs:\n",
    "        normedlogs.append(logp - logtotal) #normalise - subtracting in the log domain\n",
    "                                        #equivalent to dividing in the normal domain\n",
    "    return normedlogs\n",
    " \n",
    "def log_roulette_wheel(normedlogs):\n",
    "    \"\"\"Takes a list of normed log probabilities; returns some index of that list \n",
    "    with probability corresponding to the (exponentiated) value of that list element\"\"\"\n",
    "    r = log(random.random()) #generate a random number in [0,1), then convert to log\n",
    "    accumulator = normedlogs[0]\n",
    "    for i in range(len(normedlogs)):\n",
    "        if r < accumulator:\n",
    "            return i\n",
    "        accumulator = logsumexp([accumulator, normedlogs[i + 1]])\n",
    "\n",
    "def wta(probs):\n",
    "    \"\"\"Takes a list of probabilities (log or normal); returns the index that has the greatest probability.\"\"\"\n",
    "    maxprob = max(probs) # Find the maximum probability (works if these are logs or not)\n",
    "    candidates = []\n",
    "    for i in range(len(probs)):\n",
    "        if probs[i] == maxprob:\n",
    "            candidates.append(i) # Make a list of all the indices with that maximum probability\n",
    "    return random.choice(candidates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Production of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def produce(language, log_error_probability, n_variants):\n",
    "    \"\"\"Takes language (list of variants, represented as numbers), log_error_probability (a number), and n_variants (a number);\n",
    "       returns variable, variant pair (a 2-tuple of numbers)\"\"\"\n",
    "    variable = random.randrange(len(language)) # Pick a variable to produce\n",
    "    correct_variant = language[variable]\n",
    "    if log(random.random()) > log_error_probability:\n",
    "        return variable, correct_variant # Return the variable, variant pair\n",
    "    else:\n",
    "        possible_error_variants = list(range(n_variants))\n",
    "        possible_error_variants.remove(correct_variant)\n",
    "        error_variant = random.choice(possible_error_variants)\n",
    "        return variable, error_variant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to check if language is systematic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def systematic(language):\n",
    "    \"\"\"Takes language (list of variants, represented as numbers);\n",
    "    returns True if language is systematic, else False\"\"\"\n",
    "    first_variant = language[0]\n",
    "    for variant in language:\n",
    "        if variant != first_variant:\n",
    "            return False # The language can only be systematic if every variant is the same as the first\n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logprior(language, log_bias, n_variables, n_variants):\n",
    "    \"\"\"Takes language (list of variants, represented as numbers), log_bias \n",
    "    (log probability representing strength of preference for systematic languages),\n",
    "    the number of possible variables, and the number of possible variants;\n",
    "    returns a number, the log prior probability of the given language.\"\"\"\n",
    "    if systematic(language):\n",
    "        number_of_systematic_languages = n_variants\n",
    "        return log_bias - log(number_of_systematic_languages) #subtracting logs = dividing\n",
    "    else:\n",
    "        number_of_unsystematic_languages = n_variants ** n_variables - n_variants # the double star here means raise to the power\n",
    "                                                                         # e.g. 4 ** 2 is four squared\n",
    "        return log_subtract(0, log_bias) - log(number_of_unsystematic_languages)\n",
    "        # log(1) is 0, so log_subtract(0, bias) is equivalent to (1 - bias) in the\n",
    "        # non-log domain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loglikelihood(data, language, log_error_probability, n_variants):\n",
    "    \"\"\"Takes data (list of utterances represented as variable, variant pairs),\n",
    "    language (list of variants, represented as numbers),\n",
    "    log_error_probability, and number of variants; returns log likelihood of data given this languageg\"\"\"\n",
    "    loglikelihoods = []\n",
    "    logp_correct = log_subtract(0, log_error_probability) #logprob of producing correct form\n",
    "    logp_incorrect = log_error_probability - log(n_variants - 1) #logprob of each incorrect variant\n",
    "    for utterance in data:\n",
    "        variable = utterance[0]\n",
    "        variant = utterance[1]\n",
    "        if variant == language[variable]:\n",
    "            loglikelihoods.append(logp_correct)\n",
    "        else:\n",
    "            loglikelihoods.append(logp_incorrect)\n",
    "    return sum(loglikelihoods) #summing log likelihoods = multiplying likelihoods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn(data, log_bias, log_error_probability, learning_type, n_variables, n_variants):\n",
    "    \"\"\"\n",
    "    Takes:\n",
    "        data: list of utterances represented as variable, variant pairs\n",
    "        log_bias: log probability representing overall bias of a system toward systematic languages\n",
    "        log_error_probability: log probability of producing wrong variant\n",
    "        learning_type: either \"map\" or \"sample\"\n",
    "        n_variables: number of possible variables\n",
    "        n_variants: number of possible variants\n",
    "    Returns:\n",
    "        sampled language: A language (list of variants), chosen based on learning_type\n",
    "    \"\"\"\n",
    "    list_of_all_languages = all_languages(n_variables, n_variants)\n",
    "    list_of_posteriors = []\n",
    "    for language in list_of_all_languages:\n",
    "        this_language_posterior = loglikelihood(data, language, \n",
    "                                                log_error_probability,\n",
    "                                                n_variants) + logprior(language, log_bias, \n",
    "                                                                      n_variables, \n",
    "                                                                      n_variants)\n",
    "        list_of_posteriors.append(this_language_posterior)\n",
    "    if learning_type == 'map':\n",
    "        map_language_index = wta(list_of_posteriors) # For MAP learning, we pick the best language\n",
    "        map_language = list_of_all_languages[map_language_index]\n",
    "        return map_language\n",
    "    if learning_type == 'sample':\n",
    "        normalized_posteriors = normalize_logprobs(list_of_posteriors)\n",
    "        sampled_language_index = log_roulette_wheel(normalized_posteriors) # For sampling, we use the roulette wheel\n",
    "        sampled_language = list_of_all_languages[sampled_language_index]\n",
    "        return sampled_language"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iterated learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterate(generations, bottleneck, log_bias, log_error_probability, learning_type,\n",
    "           n_variables, n_variants):\n",
    "    \"\"\"\n",
    "    Takes:\n",
    "        generations: number of generations to run the simulation for.\n",
    "        bottleneck: number of utterances to produce in each generation.\n",
    "        log_bias: log probability representing overall bias of a system toward systematic languages\n",
    "        log_error_probability: log probability of producing wrong variant\n",
    "        learning_type: either \"map\" or \"sample\"\n",
    "        n_variables: number of possible variables\n",
    "        n_variants: number of possible variants\n",
    "    Returns: \n",
    "        accumulator: list of 0s and 1s (systematicity), one for each generation\n",
    "        language_accumulator: list of languages (themselves lists of variants).\n",
    "    \"\"\"\n",
    "    # Randomly choose a starting language and record whether or not it's systematic.\n",
    "    language = random.choice(all_languages(n_variables, n_variants))\n",
    "    if systematic(language):\n",
    "        accumulator = [1]\n",
    "    else:\n",
    "        accumulator = [0]\n",
    "    language_accumulator = [language]\n",
    "\n",
    "    # Iterate over generations.\n",
    "    for generation in range(generations):\n",
    "        data = []\n",
    "        for i in range(bottleneck):\n",
    "            data.append(produce(language, log_error_probability, n_variants))\n",
    "        language = learn(data, log_bias, log_error_probability, learning_type, \n",
    "                         n_variables, n_variants)\n",
    "        if systematic(language):\n",
    "            accumulator.append(1)\n",
    "        else:\n",
    "            accumulator.append(0)\n",
    "        language_accumulator.append(language)\n",
    "\n",
    "    return accumulator, language_accumulator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New code\n",
    "\n",
    "Imagine we have a population of individuals who share a cognitive bias and a learning strategy (i.e., sampling or MAP) that they are born with. In other words, it is encoded in their genes. These individuals transmit their linguistic behaviour culturally through iterated learning, eventually leading to a particular distribution over languages emerging. We can find that distribution for a particular combination of prior bias and learning strategy by running a long iterated learning chain, just like we were doing in the last lab.\n",
    "\n",
    "Now, imagine that there is some genetic mutation in this population and we have an individual who has a different prior and/or learning strategy. We can ask the question: will this mutation have an evolutionary advantage? In other words, will it spread through the population, or will it die out?\n",
    "\n",
    "To answer this question, we need first to think about what it means to have a survival advantage? One obvious answer is that you might have a survival advantage if you are able to learn the language of the population well. Presumably, if you learn the language of the population poorly you won't be able to communicate as well and will be at a disadvantage.\n",
    "\n",
    "The function `learning_success` allows us to estimate how well a particular type of learner will do when attempting to learn any one of a set of languages we input. The function takes the usual parameters you might expect: the bottleneck, the bias, the error probability, the type of learner (`sample` or `map`), number of variable, and number of variants. However, it also takes a list of different languages, and a number of test trials. Each test trial involves:\n",
    "\n",
    "1. picking at random one of the languages in the list, \n",
    "2. producing a number of utterances from that language (using the `bottleneck` parameter)\n",
    "3. learning a new language from that list of utterances\n",
    "4. checking whether the new language is identical to the one we originally picked (in which case we count this as a learning success)\n",
    "\n",
    "At the end it gives us the proportion of trials which were successful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learning_success(bottleneck, log_bias, log_error_probability, learning_type, \n",
    "                     n_variables, n_variants, languages, trials):\n",
    "    \"\"\"\n",
    "    Takes:\n",
    "        bottleneck: number of utterances to produce in each generation.\n",
    "        log_bias: log probability representing overall bias of a system toward systematic languages\n",
    "        log_error_probability: log probability of producing wrong variant\n",
    "        learning_type: either \"map\" or \"sample\"\n",
    "        n_variables: number of variables in the languages\n",
    "        n_variants: number of possible variants for each variable\n",
    "        languages: list of language, where each language is a list of variants, represented as numbers\n",
    "        trials: number of times to test learner on randomly sampled language\n",
    "    Returns: \n",
    "        proportion of successes out of total trials\n",
    "    \"\"\"\n",
    "    success = 0\n",
    "    for i in range(trials):\n",
    "        input_language = random.choice(languages)\n",
    "        data = []\n",
    "        for i in range(bottleneck):\n",
    "            data.append(produce(input_language, log_error_probability, n_variants))\n",
    "        output_language = learn(data, log_bias, log_error_probability, learning_type,\n",
    "                               n_variables, n_variants)\n",
    "        if output_language == input_language:\n",
    "            success = success + 1\n",
    "    return success / trials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use this function in combination with the `iterate` function to see how well a particular type of learner will learn languages that emerge from cultural evolution. For example, try the following:\n",
    "\n",
    "```\n",
    "languages = iterate(100000, 5, log(0.6), log(0.05), 'map', 2, 2)[1]\n",
    "print(learning_success(5, log(0.6), log(0.05), 'map', 2, 2, languages, 100000))\n",
    "```\n",
    "\n",
    "This will run an iterated learning simulation for 100,000 generations with a MAP learner, a bias of 0.6, and 2 variables/variants. Then it will test how well the same kind of learner learns the languages that emerge from that simulation. To get an accurate result, it runs the learning test for 100,000 trials. These two numbers (the generations and the test trials) don't need to be the same, but should ideally be quite large so that we can get accurate estimates. You can try running them with lower numbers a bunch of times and see how variable the results are to get a rough and ready idea of how accurate the samples are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, but how does this help us tell what kind of biases and learning strategies will evolve? As I discussed above, we want to see if a mutation will have an advantage (and therefore is likely to spread through a population) or not. So, really, we want to know how well a learner will do at learning, who *isn't* the same as the one that created the languages. Try this:\n",
    "\n",
    "```\n",
    "print(learning_success(5, log(0.6), log(0.05), 'sample', 2, 2, languages, 100000))\n",
    "```\n",
    "\n",
    "The original list of languages (`languages`) was created by a population of MAP learners. Now we're testing what the expected success of a learner with a sampling strategy would be if exposed to one of these languages. If this number is higher than the number we got above, then the mutation could spread through the population. If this number is lower than the number we got above, we can expect it to die out. You may find that these numbers are quite similar (which is why we need large numbers for learning trials and genenerations to get an accurate estimate). This suggests that in some cases the selection pressure on the evolution of these genes might not be enormous, but nevertheless small differences in fitness can nevertheless lead to big changes over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question\n",
    "\n",
    "There's only one question for this lab, because I want you to think about how best you can explore it with the tools I've given you here! \n",
    "\n",
    "You could answer this question just by typing in a bunch of commands like the examples above, or you could try and come up with a way of looping through different combinations. If you want, you could try and come up with a measure quantifying how big an advantage (or disadvantage) a mutation has in a particular population. If you want to be really fancy would be to then visualise these results in a graph somehow (hint: you can use `plt.imshow()` to visualise a 2-dimensional list of numbers).\n",
    "\n",
    "1. Which mutations will spread in different populations of learners, which mutations will die out, and which are selectively neutral (i.e. are neither better nor worse)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "2254d95886abd510fdbe4740a73329c02480b002321aba449e3a8ac32ef99413"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
