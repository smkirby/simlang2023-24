{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulating Language, Lab 8, Convergence to the Prior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This simulation implements a simplified version of the language model from Kirby, Dowman & Griffiths (2007) using an explicit agent-based simulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from math import log, log1p, exp\n",
    "from scipy.special import logsumexp\n",
    "\n",
    "from numpy import mean # This is a handy function that calculate the average of a list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Representing languages\n",
    "\n",
    "Following Kirby, Dowman & Griffiths (2007), we assume a language is made up of a set of *variables*, each of which can exist in a number of different *variant* forms. This is a rather general characterisation that actually applies well to a number of linguistic phenomena. For example, we can think of the variables as different syntactic categories, and the variants as word orders. Alternatively, the variables could be verb-meanings and the variants different realisations of the past tense, and so on. Agents will produce (and learn from) data which simply exemplifies which variant they have for a particular variable (with the possibility of noise on transmission). \n",
    "\n",
    "In this lab, we will group languages into two classes: systematic languages (where the same variant is used for all variables) and unsystematic languages (where more than one variant is used). Note that sometimes in the literature you'll see the words \"regular\" and \"irregular\" used instead of systematic/unsystematic but this might be a bit confusing given that we've also looked at regularisation of unpredictable variation in this course.\n",
    "\n",
    "`all_languages` enumerates all possible languages for expressing variables variables and variants variants using a cute recursive method. Please don't worry too much about how this works! However, you do need to understand what the output of this function means."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_languages(n_variables, n_variants):\n",
    "    \"\"\"Takes n_variables (number of variables in a language) and n_variants (number of variants of each variable);\n",
    "       returns list of all possible languages (all possible ways of combining the variants)\n",
    "       \"\"\"\n",
    "    if n_variables == 0:\n",
    "        return [[]] # The list of all languages with zero variables is just one language, and that's empty\n",
    "    else:\n",
    "        result = [] # If we are looking for a list of languages with more than zero variables, \n",
    "                    # then we'll need to build a list\n",
    "        smaller_langs = all_languages(n_variables - 1, n_variants) # Let's first find all the languages with one \n",
    "                                                               # fewer variables\n",
    "        for language in smaller_langs: # For each of these smaller languages, we're going to have to create a more\n",
    "                                       # complex language by adding each of the possible variants\n",
    "            for variant in range(n_variants):\n",
    "                result.append(language + [variant])\n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we run `all_languages(2, 2)` we get all possible languages with two variables and two variants. This is a list that looks like this: `[0, 0], [0, 1], [1, 0], [1, 1]]`. What does this mean exactly? Imagine a bunch of languages that were close to English but had different combinations of ways of making the plural forms for \"foot\" and \"goose\":\n",
    "\n",
    "`[[foots, gooses], [foots, geese], [feet, gooses], [feet, geese]]`\n",
    "\n",
    "Or if you had different past tense forms for \"sing\" and \"ring\":\n",
    "\n",
    "`[[singed, ringed], [singed, rang], [sang, ringed], [sang, rang]]`\n",
    "\n",
    "We're using *systematic* here to refer to a language that is systematic in the way it chooses variants for things. The first and last \"languages\" in each of these four lists is systematic in this sense. \"foots\" and \"gooses\" uses an \"s\" ending for the plural. \"feet\" and \"geese\" use the same vowel change, and so on. The others are unsystematic in that they use different ways of expressing the meanings for each meaning. In our simplified numerical notation, the `0` and `1` correspond to \"ways of expressing the plural\" or \"ways of forming the past tense\", or really any way in which a language might choose to systematically express something.\n",
    "\n",
    "Try different numbers of variants and variables in `all_languages` to make sure you understand what these numbers do to our representation of languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions for dealing with log probabilities\n",
    "\n",
    "Here are our standard functions for dealing with logs, as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_subtract(x,y):\n",
    "    \"\"\"Takes two log numbers; returns their difference.\"\"\"\n",
    "    return x + log1p(-exp(y - x))\n",
    "\n",
    "def normalize_logprobs(logprobs):\n",
    "    \"\"\"Takes a list of log numbers; returns a list of scaled versions of those numbers that, \n",
    "    once converted to probabilities, sum to 1.\"\"\"\n",
    "    logtotal = logsumexp(logprobs) #calculates the summed log probabilities\n",
    "    normedlogs = []\n",
    "    for logp in logprobs:\n",
    "        normedlogs.append(logp - logtotal) #normalise - subtracting in the log domain\n",
    "                                        #equivalent to dividing in the normal domain\n",
    "    return normedlogs\n",
    " \n",
    "def log_roulette_wheel(normedlogs):\n",
    "    \"\"\"Takes a list of normed log probabilities; returns some index of that list \n",
    "    with probability corresponding to the (exponentiated) value of that list element\"\"\"\n",
    "    r = log(random.random()) #generate a random number in [0,1), then convert to log\n",
    "    accumulator = normedlogs[0]\n",
    "    for i in range(len(normedlogs)):\n",
    "        if r < accumulator:\n",
    "            return i\n",
    "        accumulator = logsumexp([accumulator, normedlogs[i + 1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're also going to have a function that works a little like the `log_roulette_wheel()` but instead always picks the most probable index (instead of picking indices proportional to their probability). We sometimes call this a \"winner take all\" function. While `log_roulette_wheel()` can be used to implement *sampling*, `wta()` can be used to implement *MAP* hypothesis selection. If there is more than one winner, then we pick randomly among them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wta(probs):\n",
    "    \"\"\"Takes a list of probabilities (log or normal); returns the index that has the greatest probability.\"\"\"\n",
    "    maxprob = max(probs) # Find the maximum probability (works if these are logs or not)\n",
    "    candidates = []\n",
    "    for i in range(len(probs)):\n",
    "        if probs[i] == maxprob:\n",
    "            candidates.append(i) # Make a list of all the indices with that maximum probability\n",
    "    return random.choice(candidates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Production of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def produce(language, log_error_probability):\n",
    "    \"\"\"Takes language (list of variants, represented as numbers) and log_error_probability (a number);\n",
    "       returns variable, variant pair (a 2-tuple of numbers)\"\"\"\n",
    "    n_variants = len(language) # the number of possible variants is simply the length of the language\n",
    "    variable = random.randrange(len(language)) # Pick a variable to produce\n",
    "    correct_variant = language[variable]\n",
    "    if log(random.random()) > log_error_probability:\n",
    "        return variable, correct_variant # Return the variable, variant pair\n",
    "    else:\n",
    "        possible_error_variants = list(range(n_variants))\n",
    "        possible_error_variants.remove(correct_variant)\n",
    "        error_variant = random.choice(possible_error_variants)\n",
    "        return variable, error_variant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function produce takes a language, selects a random variable, and produces the relevant variant from the language, with a certain probability of error. Can you see how errors on production work?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Stepping through the code, we first see an if statement which checks some log probabilities to make a decision. Let's ignore their loggyness as they're not important here.\n",
    "- We check whether `random.random()` - a random number between 0 and 1 - is bigger than (log)`error_probability`, which is usually quite small, say 0.001 for example.\n",
    "- If `error_probability` is quite small, then `random.random` will almost always be bigger, and in this case the function just returns the intended `variable`/`variant` pair and the function exits.\n",
    "- In the rare case that it's actually smaller than `error_probability`, then... there's been an error! Then we go into the `else` condition, which will tell us what to do.\n",
    "- We see that `possible_error_variants` creates a list with the same length as the number of possible `variant`s, and goes on to `remove` the `correct_variant` \n",
    "- This means we will pick an `error_variant` which can be anything *except* the intended variant, which makes sense.\n",
    "- `random.choice` now comes out to play, which simply picks a member of a list at random. In this case, it will pick an `error_variant`, whih can be any of the variants except the intended one.\n",
    "- Finally, the function exits and returns a pair with the correct `variable` but the *wrong* `variant`, and the sneaky error has entered the system. That's it!*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifying languages\n",
    "\n",
    "In this language model, prior probability is determined by language class: systematic languages differ from unsystematic languages in their prior probability, and ultimately we are interested in the proportion of our simulated population who use systematic languages. We therefore need a function to take a language and classify it as systematic or not – the function `systematic` does this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def systematic(language):\n",
    "    \"\"\"Takes language (list of variants, represented as numbers);\n",
    "    returns True if language is systematic, else False\"\"\"\n",
    "    first_variant = language[0]\n",
    "    for variant in language:\n",
    "        if variant != first_variant:\n",
    "            return False # The language can only be systematic if every variant is the same as the first\n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The purpose of this function is to check whether a particular language is systematic or not. The function uses the fact that we know that, in a completely systematic language, every `variable` must be the same type of `variant`. \n",
    "* So all we need to do here is check that every item in the list is the same type, i.e. just a list of the same value repeated every time. \n",
    "* There are a lot of ways that we *could* do this, but the one used here is sensible: \n",
    "    * `first_variant` records the value of the first item in the `language` list by checking index `0`\n",
    "    * The for loop sets up a check for every `variant` in the `language` list\n",
    "    * If at any point the `variant` being checked is not the same as `first_variant`, that means that the language has more than one value in it.\n",
    "    * In code language, if `variant != first_variant`, then we immediately know that the language is not systematic, and can exit with an authoritative `False`, as there is no need to check any more items.\n",
    "    * On the other hand, if we manage to get all the way to the end of the list without finding any non-conformist variants, we finish the for loop knowing that the language must be systematic, and can exit with `True`, hurrah"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Bayesian bits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logprior(language, log_bias, n_variables, n_variants):\n",
    "    \"\"\"Takes language (list of variants, represented as numbers), log_bias \n",
    "    (log probability representing strength of preference for systematic languages),\n",
    "    the number of possible variables, and the number of possible variants;\n",
    "    returns a number, the log prior probability of the given language.\"\"\"\n",
    "    if systematic(language):\n",
    "        number_of_systematic_languages = n_variants\n",
    "        return log_bias - log(number_of_systematic_languages) #subtracting logs = dividing\n",
    "    else:\n",
    "        number_of_unsystematic_languages = n_variants ** n_variables - n_variants # the double star here means raise to the power\n",
    "                                                                         # e.g. 4 ** 2 is four squared\n",
    "        return log_subtract(0, log_bias) - log(number_of_unsystematic_languages)\n",
    "        # log(1) is 0, so log_subtract(0, bias) is equivalent to (1 - bias) in the\n",
    "        # non-log domain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function `logprior` returns the prior probability (as a log probability) of a particular language. The strength of preference for systematic languages is given as the second argument – if bias is over 0.5 (when converted back from a log probability), systematic languages have higher prior probability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We know that a systematic language is just a repeated list of the same value.\n",
    "* This means there is only one possible systematic language for each variant: if languages are represented by a list of 5 variants, and the only variants are `0` and `1`, then the possible systematic languages would be `[0,0,0,0,0]` and `[1,1,1,1,1]`.\n",
    "* So the number of possible systematic languages is just the same as the number of variants, as we see in `number_of_systematic_languages = variants`\n",
    "* How many *unsystematic* languages are there? Well, first we can calculate the total number of possible languages overall...\n",
    "* This is just like counting outcomes of a coin-flip or a dice-roll: the number of possibilities for the first variable (coin-flip/dice-roll), times the number of possibilities for the second, times etc etc\n",
    "* So say languages are represented by 5 `variable`s, and there are 3 `variant`s, the total number of possible languages is `3 * 3 * 3 * 3 * 3`\n",
    "* A quicker way of writing that is $3^5$, or in Python `3**5`\n",
    "* So the total number of possible langages is `variants ** variables`\n",
    "* BUT: that includes all systematic **and** unsystematic languages.\n",
    "* But since we know that the number of *systematic* languages is just the same as the number of `variants`, we can just subtract that from the total number, hence `number_of_unsystematic_languages = variants ** variables - variants`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Why are we dividing the bias by the number of systematic and unsystematic languages in this function? Check you understand how these numbers are calculated.\n",
    "- How does this function differ from the prior from the Kirby, Dowman & Griffiths (2007) paper? (Hint: consider the case of more than two variables.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loglikelihood(data, language, log_error_probability, n_variants):\n",
    "    \"\"\"Takes data (list of utterances represented as variable, variant pairs),\n",
    "    language (list of variants, represented as numbers),\n",
    "    log_error_probability, and number of variants; returns log likelihood of data given this languageg\"\"\"\n",
    "    loglikelihoods = []\n",
    "    logp_correct = log_subtract(0, log_error_probability) #logprob of producing correct form\n",
    "    logp_incorrect = log_error_probability - log(n_variants - 1) #logprob of each incorrect variant\n",
    "    for utterance in data:\n",
    "        variable = utterance[0]\n",
    "        variant = utterance[1]\n",
    "        if variant == language[variable]:\n",
    "            loglikelihoods.append(logp_correct)\n",
    "        else:\n",
    "            loglikelihoods.append(logp_incorrect)\n",
    "    return sum(loglikelihoods) #summing log likelihoods = multiplying likelihoods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function `loglikelihood` takes a language and a list of data and works out the (log) likelihood of the data given the language. We allows some small probability (given by the third argument) that a speaker will produce the ‘wrong’ variant, i.e. a variant other than that specified by their language."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning\n",
    "\n",
    "Bayesian learners calculate the posterior probability of each language based on some data, then select a language (‘learn’) based on those posterior probabilities. `learn` implements this. As discussed in the lecture, there are two ways you could select a language based on the posterior probability distribution:\n",
    "- You could pick the best language – i.e. the language with the highest posterior probability. This is called MAP (“maximum a posteriori”) learning.\n",
    "- Alternatively, you could pick a language probabilistically based on its posterior probability, without necessarily going for the best one every time (e.g. if language 0 has twice the posterior probability of language 1, you are twice as likely to pick it). This is called sampling (for “sampling from the posterior distribution”).\n",
    "\n",
    "The `learn` function implements both these ways of learning, using the `wta` function to do MAP learning and using `log_roulette_wheel` to do sampling (from previous labs, which assumed learners sample from the posterior)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn(data, log_bias, log_error_probability, learning_type, n_variables, n_variants):\n",
    "    \"\"\"\n",
    "    Takes:\n",
    "        data: list of utterances represented as variable, variant pairs\n",
    "        log_bias: log probability representing overall bias of a system toward systematic languages\n",
    "        log_error_probability: log probability of producing wrong variant\n",
    "        learning_type: either \"map\" or \"sample\"\n",
    "        n_variables: number of possible variables\n",
    "        n_variants: number of possible variants\n",
    "    Returns:\n",
    "        sampled language: A language (list of variants), chosen based on learning_type\n",
    "    \"\"\"\n",
    "    list_of_all_languages = all_languages(n_variables, n_variants)\n",
    "    list_of_posteriors = []\n",
    "    for language in list_of_all_languages:\n",
    "        this_language_posterior = loglikelihood(data, language, \n",
    "                                                log_error_probability,\n",
    "                                                n_variants) + logprior(language, log_bias, \n",
    "                                                                      n_variables, \n",
    "                                                                      n_variants)\n",
    "        list_of_posteriors.append(this_language_posterior)\n",
    "    if learning_type == 'map':\n",
    "        map_language_index = wta(list_of_posteriors) # For MAP learning, we pick the best language\n",
    "        map_language = list_of_all_languages[map_language_index]\n",
    "        return map_language\n",
    "    if learning_type == 'sample':\n",
    "        normalized_posteriors = normalize_logprobs(list_of_posteriors)\n",
    "        sampled_language_index = log_roulette_wheel(normalized_posteriors) # For sampling, we use the roulette wheel\n",
    "        sampled_language = list_of_all_languages[sampled_language_index]\n",
    "        return sampled_language"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterated learning\n",
    "\n",
    "`iterate()` is the top-level function which actually runs the simulation. It starts with a random language from the set of possible languages, and then each generation a learner learns from data produced by the previous agent. This function returns a list indicating whether the language is systematic or not each generation. For convenience we encode the systematic languages as `1` in this list and `0` otherwise. It also returns a second list, showing what each language was per generation in case you want more details. (Generally, we're just going to be using the first list for plotting graphs etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterate(generations, bottleneck, log_bias, log_error_probability, learning_type,\n",
    "           n_variables, n_variants):\n",
    "    \"\"\"\n",
    "    Takes:\n",
    "        generations: number of generations to run the simulation for.\n",
    "        bottleneck: number of utterances to produce in each generation.\n",
    "        log_bias: log probability representing overall bias of a system toward systematic languages\n",
    "        log_error_probability: log probability of producing wrong variant\n",
    "        learning_type: either \"map\" or \"sample\"\n",
    "        n_variables: number of possible variables\n",
    "        n_variants: number of possible variants\n",
    "    Returns: \n",
    "        accumulator: list of 0s and 1s (systematicity), one for each generation\n",
    "        language_accumulator: list of languages (themselves lists of variants).\n",
    "    \"\"\"\n",
    "    # Randomly choose a starting language and record whether or not it's systematic.\n",
    "    language = random.choice(all_languages(n_variables, n_variants))\n",
    "    if systematic(language):\n",
    "        accumulator = [1]\n",
    "    else:\n",
    "        accumulator = [0]\n",
    "    language_accumulator = [language]\n",
    "\n",
    "    # Iterate over generations.\n",
    "    for generation in range(generations):\n",
    "        data = []\n",
    "        for i in range(bottleneck):\n",
    "            data.append(produce(language, log_error_probability))\n",
    "        language = learn(data, log_bias, log_error_probability, learning_type, \n",
    "                         n_variables, n_variants)\n",
    "        if systematic(language):\n",
    "            accumulator.append(1)\n",
    "        else:\n",
    "            accumulator.append(0)\n",
    "        language_accumulator.append(language)\n",
    "\n",
    "    return accumulator, language_accumulator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, for example, to do an iterated learning simulation for 10 generations, a bottleneck of 5, with a bias in favour of systematicity of 0.6, an error probability of 0.05, with sampling as the learning type, 2 variables, and 2 variants, you'd use:\n",
    "\n",
    "```\n",
    "iterate(10, 5, log(0.6), log(0.05), 'sample', 2, 2)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions\n",
    "\n",
    "**Note:** To answer some of these questions, you're going to have to run quite long simulation runs (up to 100000 generations to get super accurate results). In general, you probably want to keep the bottleneck values between 1 and 10.\n",
    "\n",
    "1. Using the default parameters suggested above, can you demonstrate convergence to the prior? You may want to start with a line graph of the results to get a sense of what's going on in the simulation. You could also plot a histogram to get the overall proportion of systematic languages in the whole run, but notice that this is just the same as the average of your whole list so you don't really need a graph for this! You can get the average directly by using, for example, `print(mean(data[0]))` if `data` is the result of your simulation run (remember the `iterate()` function returns whether the language at the each generation is systematic or not as the first part of the data it returns, which is why we use `data[0]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. How does changing the error rate and bottleneck size affect the results for the sample learner? Make sure you run the simulation long enough to get repeatable results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Switch the learning type to MAP learning, and rerun the simulation. Can you show the amplification of prior bias that is shown in the paper?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. How is bias amplification affected by the bottleneck size? Can you run a range of simulations for different bottleneck sizes, find the means, and plot these nicely in a graph?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "2254d95886abd510fdbe4740a73329c02480b002321aba449e3a8ac32ef99413"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
