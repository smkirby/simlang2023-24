{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulating Language, Lab 9, Gene-culture co-evolution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to use the same code as the last lab to do something similar to Smith & Kirby (2008) and discover what types of prior and learning strategy combinations are evolutionarily stable. You may be surprised to find that we really don't need much more than the code we already have to do this!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code from Lab 8\n",
    "\n",
    "Here's the code from Lab 8, with no changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from math import log, log1p, exp\n",
    "from scipy.special import logsumexp\n",
    "\n",
    "from numpy import mean # This is a handy function that calculate the average of a list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup space of possible languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_languages(n_variables, n_variants):\n",
    "    \"\"\"Takes n_variables (number of variables in a language) and n_variants (number of variants of each variable);\n",
    "       returns list of all possible languages (all possible ways of combining the variants)\n",
    "       \"\"\"\n",
    "    if n_variables == 0:\n",
    "        return [[]] # The list of all languages with zero variables is just one language, and that's empty\n",
    "    else:\n",
    "        result = [] # If we are looking for a list of languages with more than zero variables, \n",
    "                    # then we'll need to build a list\n",
    "        smaller_langs = all_languages(n_variables - 1, n_variants) # Let's first find all the languages with one \n",
    "                                                               # fewer variables\n",
    "        for language in smaller_langs: # For each of these smaller languages, we're going to have to create a more\n",
    "                                       # complex language by adding each of the possible variants\n",
    "            for variant in range(n_variants):\n",
    "                result.append(language + [variant])\n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log probability functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_subtract(x,y):\n",
    "    \"\"\"Takes two log numbers; returns their difference.\"\"\"\n",
    "    return x + log1p(-exp(y - x))\n",
    "\n",
    "def normalize_logprobs(logprobs):\n",
    "    \"\"\"Takes a list of log numbers; returns a list of scaled versions of those numbers that, \n",
    "    once converted to probabilities, sum to 1.\"\"\"\n",
    "    logtotal = logsumexp(logprobs) #calculates the summed log probabilities\n",
    "    normedlogs = []\n",
    "    for logp in logprobs:\n",
    "        normedlogs.append(logp - logtotal) #normalise - subtracting in the log domain\n",
    "                                        #equivalent to dividing in the normal domain\n",
    "    return normedlogs\n",
    " \n",
    "def log_roulette_wheel(normedlogs):\n",
    "    \"\"\"Takes a list of normed log probabilities; returns some index of that list \n",
    "    with probability corresponding to the (exponentiated) value of that list element\"\"\"\n",
    "    r = log(random.random()) #generate a random number in [0,1), then convert to log\n",
    "    accumulator = normedlogs[0]\n",
    "    for i in range(len(normedlogs)):\n",
    "        if r < accumulator:\n",
    "            return i\n",
    "        accumulator = logsumexp([accumulator, normedlogs[i + 1]])\n",
    "\n",
    "def wta(probs):\n",
    "    \"\"\"Takes a list of probabilities (log or normal); returns the index that has the greatest probability.\"\"\"\n",
    "    maxprob = max(probs) # Find the maximum probability (works if these are logs or not)\n",
    "    candidates = []\n",
    "    for i in range(len(probs)):\n",
    "        if probs[i] == maxprob:\n",
    "            candidates.append(i) # Make a list of all the indices with that maximum probability\n",
    "    return random.choice(candidates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Production of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def produce(language, log_error_probability, n_variants):\n",
    "    \"\"\"Takes language (list of variants, represented as numbers), log_error_probability (a number), and n_variants (a number);\n",
    "       returns variable, variant pair (a 2-tuple of numbers)\"\"\"\n",
    "    variable = random.randrange(len(language)) # Pick a variable to produce\n",
    "    correct_variant = language[variable]\n",
    "    if log(random.random()) > log_error_probability:\n",
    "        return variable, correct_variant # Return the variable, variant pair\n",
    "    else:\n",
    "        possible_error_variants = list(range(n_variants))\n",
    "        possible_error_variants.remove(correct_variant)\n",
    "        error_variant = random.choice(possible_error_variants)\n",
    "        return variable, error_variant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to check if language is systematic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def systematic(language):\n",
    "    \"\"\"Takes language (list of variants, represented as numbers);\n",
    "    returns True if language is systematic, else False\"\"\"\n",
    "    first_variant = language[0]\n",
    "    for variant in language:\n",
    "        if variant != first_variant:\n",
    "            return False # The language can only be systematic if every variant is the same as the first\n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logprior(language, log_bias, n_variables, n_variants):\n",
    "    \"\"\"Takes language (list of variants, represented as numbers), log_bias \n",
    "    (log probability representing strength of preference for systematic languages),\n",
    "    the number of possible variables, and the number of possible variants;\n",
    "    returns a number, the log prior probability of the given language.\"\"\"\n",
    "    if systematic(language):\n",
    "        number_of_systematic_languages = n_variants\n",
    "        return log_bias - log(number_of_systematic_languages) #subtracting logs = dividing\n",
    "    else:\n",
    "        number_of_unsystematic_languages = n_variants ** n_variables - n_variants # the double star here means raise to the power\n",
    "                                                                         # e.g. 4 ** 2 is four squared\n",
    "        return log_subtract(0, log_bias) - log(number_of_unsystematic_languages)\n",
    "        # log(1) is 0, so log_subtract(0, bias) is equivalent to (1 - bias) in the\n",
    "        # non-log domain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loglikelihood(data, language, log_error_probability, n_variants):\n",
    "    \"\"\"Takes data (list of utterances represented as variable, variant pairs),\n",
    "    language (list of variants, represented as numbers),\n",
    "    log_error_probability, and number of variants; returns log likelihood of data given this languageg\"\"\"\n",
    "    loglikelihoods = []\n",
    "    logp_correct = log_subtract(0, log_error_probability) #logprob of producing correct form\n",
    "    logp_incorrect = log_error_probability - log(n_variants - 1) #logprob of each incorrect variant\n",
    "    for utterance in data:\n",
    "        variable = utterance[0]\n",
    "        variant = utterance[1]\n",
    "        if variant == language[variable]:\n",
    "            loglikelihoods.append(logp_correct)\n",
    "        else:\n",
    "            loglikelihoods.append(logp_incorrect)\n",
    "    return sum(loglikelihoods) #summing log likelihoods = multiplying likelihoods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn(data, log_bias, log_error_probability, learning_type, n_variables, n_variants):\n",
    "    \"\"\"\n",
    "    Takes:\n",
    "        data: list of utterances represented as variable, variant pairs\n",
    "        log_bias: log probability representing overall bias of a system toward systematic languages\n",
    "        log_error_probability: log probability of producing wrong variant\n",
    "        learning_type: either \"map\" or \"sample\"\n",
    "        n_variables: number of possible variables\n",
    "        n_variants: number of possible variants\n",
    "    Returns:\n",
    "        sampled language: A language (list of variants), chosen based on learning_type\n",
    "    \"\"\"\n",
    "    list_of_all_languages = all_languages(n_variables, n_variants)\n",
    "    list_of_posteriors = []\n",
    "    for language in list_of_all_languages:\n",
    "        this_language_posterior = loglikelihood(data, language, \n",
    "                                                log_error_probability,\n",
    "                                                n_variants) + logprior(language, log_bias, \n",
    "                                                                      n_variables, \n",
    "                                                                      n_variants)\n",
    "        list_of_posteriors.append(this_language_posterior)\n",
    "    if learning_type == 'map':\n",
    "        map_language_index = wta(list_of_posteriors) # For MAP learning, we pick the best language\n",
    "        map_language = list_of_all_languages[map_language_index]\n",
    "        return map_language\n",
    "    if learning_type == 'sample':\n",
    "        normalized_posteriors = normalize_logprobs(list_of_posteriors)\n",
    "        sampled_language_index = log_roulette_wheel(normalized_posteriors) # For sampling, we use the roulette wheel\n",
    "        sampled_language = list_of_all_languages[sampled_language_index]\n",
    "        return sampled_language"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iterated learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterate(generations, bottleneck, log_bias, log_error_probability, learning_type,\n",
    "           n_variables, n_variants):\n",
    "    \"\"\"\n",
    "    Takes:\n",
    "        generations: number of generations to run the simulation for.\n",
    "        bottleneck: number of utterances to produce in each generation.\n",
    "        log_bias: log probability representing overall bias of a system toward systematic languages\n",
    "        log_error_probability: log probability of producing wrong variant\n",
    "        learning_type: either \"map\" or \"sample\"\n",
    "        n_variables: number of possible variables\n",
    "        n_variants: number of possible variants\n",
    "    Returns: \n",
    "        accumulator: list of 0s and 1s (systematicity), one for each generation\n",
    "        language_accumulator: list of languages (themselves lists of variants).\n",
    "    \"\"\"\n",
    "    # Randomly choose a starting language and record whether or not it's systematic.\n",
    "    language = random.choice(all_languages(n_variables, n_variants))\n",
    "    if systematic(language):\n",
    "        accumulator = [1]\n",
    "    else:\n",
    "        accumulator = [0]\n",
    "    language_accumulator = [language]\n",
    "\n",
    "    # Iterate over generations.\n",
    "    for generation in range(generations):\n",
    "        data = []\n",
    "        for i in range(bottleneck):\n",
    "            data.append(produce(language, log_error_probability, n_variants))\n",
    "        language = learn(data, log_bias, log_error_probability, learning_type, \n",
    "                         n_variables, n_variants)\n",
    "        if systematic(language):\n",
    "            accumulator.append(1)\n",
    "        else:\n",
    "            accumulator.append(0)\n",
    "        language_accumulator.append(language)\n",
    "\n",
    "    return accumulator, language_accumulator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New code\n",
    "\n",
    "Imagine we have a population of individuals who share a cognitive bias and a learning strategy (i.e., sampling or MAP) that they are born with. In other words, it is encoded in their genes. These individuals transmit their linguistic behaviour culturally through iterated learning, eventually leading to a particular distribution over languages emerging. We can find that distribution for a particular combination of prior bias and learning strategy by running a long iterated learning chain, just like we were doing in the last lab.\n",
    "\n",
    "Now, imagine that there is some genetic mutation in this population and we have an individual who has a different prior and/or learning strategy. We can ask the question: will this mutation have an evolutionary advantage? In other words, will it spread through the population, or will it die out?\n",
    "\n",
    "To answer this question, we need first to think about what it means to have a survival advantage? One obvious answer is that you might have a survival advantage if you are able to learn the language of the population well. Presumably, if you learn the language of the population poorly you won't be able to communicate as well and will be at a disadvantage.\n",
    "\n",
    "The function `learning_success` allows us to estimate how well a particular type of learner will do when attempting to learn any one of a set of languages we input. The function takes the usual parameters you might expect: the bottleneck, the bias, the error probability, the type of learner (`sample` or `map`), number of variable, and number of variants. However, it also takes a list of different languages, and a number of test trials. Each test trial involves:\n",
    "\n",
    "1. picking at random one of the languages in the list, \n",
    "2. producing a number of utterances from that language (using the `bottleneck` parameter)\n",
    "3. learning a new language from that list of utterances\n",
    "4. checking whether the new language is identical to the one we originally picked (in which case we count this as a learning success)\n",
    "\n",
    "At the end it gives us the proportion of trials which were successful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learning_success(bottleneck, log_bias, log_error_probability, learning_type, \n",
    "                     n_variables, n_variants, languages, trials):\n",
    "    \"\"\"\n",
    "    Takes:\n",
    "        bottleneck: number of utterances to produce in each generation.\n",
    "        log_bias: log probability representing overall bias of a system toward systematic languages\n",
    "        log_error_probability: log probability of producing wrong variant\n",
    "        learning_type: either \"map\" or \"sample\"\n",
    "        n_variables: number of variables in the languages\n",
    "        n_variants: number of possible variants for each variable\n",
    "        languages: list of language, where each language is a list of variants, represented as numbers\n",
    "        trials: number of times to test learner on randomly sampled language\n",
    "    Returns: \n",
    "        proportion of successes out of total trials\n",
    "    \"\"\"\n",
    "    success = 0\n",
    "    for i in range(trials):\n",
    "        input_language = random.choice(languages)\n",
    "        data = []\n",
    "        for i in range(bottleneck):\n",
    "            data.append(produce(input_language, log_error_probability, n_variants))\n",
    "        output_language = learn(data, log_bias, log_error_probability, learning_type,\n",
    "                               n_variables, n_variants)\n",
    "        if output_language == input_language:\n",
    "            success = success + 1\n",
    "    return success / trials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use this function in combination with the `iterate` function to see how well a particular type of learner will learn languages that emerge from cultural evolution. For example, try the following:\n",
    "\n",
    "```\n",
    "languages = iterate(100000, 5, log(0.6), log(0.05), 'map', 2, 2)[1]\n",
    "print(learning_success(5, log(0.6), log(0.05), 'map', 2, 2, languages, 100000))\n",
    "```\n",
    "\n",
    "This will run an iterated learning simulation for 100,000 generations with a MAP learner, a bias of 0.6, and 2 variables/variants. Then it will test how well the same kind of learner learns the languages that emerge from that simulation. To get an accurate result, it runs the learning test for 100,000 trials. These two numbers (the generations and the test trials) don't need to be the same, but should ideally be quite large so that we can get accurate estimates. You can try running them with lower numbers a bunch of times and see how variable the results are to get a rough and ready idea of how accurate the samples are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.96257\n"
     ]
    }
   ],
   "source": [
    "languages = iterate(100000, 5, log(0.6), log(0.05), 'map', 2, 2)[1]\n",
    "print(learning_success(5, log(0.6), log(0.05), 'map', 2, 2, languages, 100000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, but how does this help us tell what kind of biases and learning strategies will evolve? As I discussed above, we want to see if a mutation will have an advantage (and therefore is likely to spread through a population) or not. So, really, we want to know how well a learner will do at learning, who *isn't* the same as the one that created the languages. Try this:\n",
    "\n",
    "```\n",
    "print(learning_success(5, log(0.6), log(0.05), 'sample', 2, 2, languages, 100000))\n",
    "```\n",
    "\n",
    "The original list of languages (`languages`) was created by a population of MAP learners. Now we're testing what the expected success of a learner with a sampling strategy would be if exposed to one of these languages. If this number is higher than the number we got above, then the mutation could spread through the population. If this number is lower than the number we got above, we can expect it to die out. You may find that these numbers are quite similar (which is why we need large numbers for learning trials and genenerations to get an accurate estimate). This suggests that in some cases the selection pressure on the evolution of these genes might not be enormous, but nevertheless small differences in fitness can nevertheless lead to big changes over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.90935\n"
     ]
    }
   ],
   "source": [
    "print(learning_success(5, log(0.6), log(0.05), 'sample', 2, 2, languages, 100000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question\n",
    "\n",
    "There's only one question for this lab, because I want you to think about how best you can explore it with the tools I've given you here! \n",
    "\n",
    "You could answer this question just by typing in a bunch of commands like the examples above, or you could try and come up with a way of looping through different combinations. If you want, you could try and come up with a measure quantifying how big an advantage (or disadvantage) a mutation has in a particular population. If you want to be really fancy would be to then visualise these results in a graph somehow (hint: you can use `plt.imshow()` to visualise a 2-dimensional list of numbers).\n",
    "\n",
    "1. Which mutations will spread in different populations of learners, which mutations will die out, and which are selectively neutral (i.e. are neither better nor worse)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*My approach to this is going to be to try three different prior biases, from very weak to very strong, plus the two types of learner (sample vs. map). So first up, for each of these combinations we'll run a long simulation to gather the set of languages that would emerge in a population with that learning strategy/bias combination. Just to keep things neat, let's write a function to do that.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_stationary_distributions(bias_learning_type_pairs, n_variables, n_variants):\n",
    "    \"\"\"\n",
    "    Takes bias_learning_type_pairs (a tuple consisting of bias (a number between 0 and 1,\n",
    "    reflecting strength of preference for systematicity) and learning_type ('map' or 'sample')),\n",
    "    the number of variables, and the number of variants.\n",
    "    Returns list of languages produced during iterated learning with each bias and learning\n",
    "    type. I've baked in 2 variables and 2 variants into this function just to keep things\n",
    "    simple.\n",
    "    \"\"\"\n",
    "    stationary_distributions = []\n",
    "    for bias, learning_type in bias_learning_type_pairs:\n",
    "        print(bias, learning_type)\n",
    "        languages = iterate(100000, 5, log(bias), log(0.05), learning_type, \n",
    "                            n_variables, n_variants)[1]\n",
    "        stationary_distributions.append(languages)\n",
    "    return stationary_distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*This function I've just defined takes a list of bias, learning type pairs and runs a long simulation for each of them. You can think of a combination of a learning bias and a learning type (i.e. hypothesis selection strategy) as characterising a learner – it's what we assume is innate, and therefore provided by evolution. Let's choose a range of biases in favour of systematicity from relatively weak (near 0.5) to relatively strong (near 1.0) and run these for both sample and map. This list below gives these different possible learners.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "learners = [(0.6, 'sample'), (0.7, 'sample'), (0.8, 'sample'),\n",
    "            (0.6, 'map'), (0.7, 'map'), (0.8, 'map')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Now we use this list and the function I defined to generate a list of stationary distributions (i.e. a list of languages) for each of these. **Strictly speaking, these aren't exactly the stationary distributions** since it should take some time for the culturally evolving system to settle into the stationary distribution. In other words, it'll take some time for the influence of the first language to be \"washed out\". However, since we're running for 100,000 generations, we can probably ignore this. (But maybe it would be better to change this to look only at the second half of the run?). For some values of bias (very high or very low), you may need to run longer simulations (both here and when evaluating learning in the next step) before you get accurate values, so please do bear that in mind!*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6 sample\n",
      "0.7 sample\n",
      "0.8 sample\n",
      "0.6 map\n",
      "0.7 map\n",
      "0.8 map\n"
     ]
    }
   ],
   "source": [
    "stationary_distributions = generate_stationary_distributions(learners)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Now we need to test each of our six learners on each of these six distributions. This corresponds to how well a \"mutant\" learner will fare in a majority learner's culture. Here's a function to do this, which will give the result as a table (actually a list of lists). Each row of the table will correspond to the mutant learner, and each column will be the stationary distribution (i.e. the majority learner).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6 sample\n",
      "0.7 sample\n",
      "0.8 sample\n",
      "0.6 map\n",
      "0.7 map\n",
      "0.8 map\n"
     ]
    }
   ],
   "source": [
    "def table_of_success(bias_learning_type_pairs, stationary_distributions, n_variables, n_variants):\n",
    "    \"\"\"\n",
    "    Takes:\n",
    "        bias_learning_type_pairs: a tuple consisting of bias (a number between 0 and 1,\n",
    "            reflecting strength of preference for systematicity) and learning_type ('map' or 'sample')\n",
    "        stationary_distributions: list of stationary distributions (one for each pair in bias_learning_type_pairs), \n",
    "            each of which is a list of languages (where one language is a list of variants)\n",
    "        n_variables: the number of variables\n",
    "        n_variants: the number of variants\n",
    "    Returns:\n",
    "        A list of lists, one for each pair in bias_learning_type_pairs. Each list contains the\n",
    "        learning success of a learner with that bias/learning type combination on all languages \n",
    "        in the stationary distribution.\n",
    "    \"\"\"\n",
    "    table = []\n",
    "    for bias, learning_type in bias_learning_type_pairs:\n",
    "        print(bias, learning_type)\n",
    "        table_row = []\n",
    "        for languages in stationary_distributions:\n",
    "            success = learning_success(5, log(bias), log(0.05), learning_type, \n",
    "                                       n_variables, n_variants, languages, 100000)\n",
    "            table_row.append(success)\n",
    "        table.append(table_row)\n",
    "    return table\n",
    "\n",
    "results = table_of_success(learners, stationary_distributions, 2, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Let's look at those results... we'll start by just printing the table out, then trying to print it a bit more neatly!*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.89897, 0.90188, 0.90685, 0.91042, 0.91088, 0.9089], [0.89999, 0.90685, 0.91532, 0.92234, 0.92241, 0.92177], [0.8931, 0.90755, 0.92166, 0.9331, 0.93175, 0.93321], [0.92934, 0.93854, 0.9519, 0.96195, 0.96282, 0.96251], [0.92957, 0.93885, 0.95306, 0.96196, 0.96306, 0.96211], [0.92816, 0.93956, 0.95364, 0.96221, 0.96263, 0.96204]]\n"
     ]
    }
   ],
   "source": [
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.89897\t0.90188\t0.90685\t0.91042\t0.91088\t0.9089\t\n",
      "\n",
      "0.89999\t0.90685\t0.91532\t0.92234\t0.92241\t0.92177\t\n",
      "\n",
      "0.8931\t0.90755\t0.92166\t0.9331\t0.93175\t0.93321\t\n",
      "\n",
      "0.92934\t0.93854\t0.9519\t0.96195\t0.96282\t0.96251\t\n",
      "\n",
      "0.92957\t0.93885\t0.95306\t0.96196\t0.96306\t0.96211\t\n",
      "\n",
      "0.92816\t0.93956\t0.95364\t0.96221\t0.96263\t0.96204\t\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for row in results:\n",
    "    for cell in row:\n",
    "        print(cell, end='\\t') # this prints with a tab instead of a new line\n",
    "    print('\\n') # this prints a newline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Let's try and visualise these a bit better. Here's my first attempt, with `plt.imshow()`.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x15004a970>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*If I get a graph that looks useful, I then go to the matplotlib website and try and figure out how to make it more useful... This was a bit fiddly, but here's what I came up with after reading that website and googling around a bit :-)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.colorbar.Colorbar at 0x150dea670>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(1, 1)\n",
    "fig = ax.imshow(results, extent=[0,6,6,0], cmap='coolwarm')\n",
    "\n",
    "labels = ['.6 S', '.7 S', '.8 S', '.6 M', '.7 M', '.8 M']\n",
    "\n",
    "ax.set_xticks([.5,1.5,2.5,3.5,4.5,5.5])\n",
    "ax.set_xticklabels(labels)\n",
    "\n",
    "ax.set_yticks([.5,1.5,2.5,3.5,4.5,5.5])\n",
    "ax.set_yticklabels(labels)\n",
    "\n",
    "ax.set_ylabel(\"Mutant\")\n",
    "ax.set_xlabel(\"Majority\")\n",
    "\n",
    "plt.colorbar(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*So, it looks like there are general differences in strategy, with MAP learners learning better than samplers. But really, we want to know is not the overall learning success, but whether a mutant learner is better than the majority learner in the population into which it is born. If it is better, then it has a chance of taking over the population. To figure this out we need to know how well the learner will do if born into a population of other learners who are the same and then compare a mutant to this. If you think about it, this is the diagonal of the table above (i.e. when the mutant **is** the learner that created the stationary distribution). We can extract this as follows:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.89897, 0.90685, 0.92166, 0.96195, 0.96306, 0.96204]\n"
     ]
    }
   ],
   "source": [
    "self_learning = []\n",
    "for i in range(6):\n",
    "    self_learning.append(results[i][i])\n",
    "print(self_learning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Now we can compare each cell in the table and see if the learning success for the mutant is higher than the non-mutant, lower or the same.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.6, 'sample') dies out in a population of (0.7, 'sample')\n",
      "(0.6, 'sample') dies out in a population of (0.8, 'sample')\n",
      "(0.6, 'sample') dies out in a population of (0.6, 'map')\n",
      "(0.6, 'sample') dies out in a population of (0.7, 'map')\n",
      "(0.6, 'sample') dies out in a population of (0.8, 'map')\n",
      "(0.7, 'sample') invades a population of (0.6, 'sample')\n",
      "(0.7, 'sample') dies out in a population of (0.8, 'sample')\n",
      "(0.7, 'sample') dies out in a population of (0.6, 'map')\n",
      "(0.7, 'sample') dies out in a population of (0.7, 'map')\n",
      "(0.7, 'sample') dies out in a population of (0.8, 'map')\n",
      "(0.8, 'sample') dies out in a population of (0.6, 'sample')\n",
      "(0.8, 'sample') invades a population of (0.7, 'sample')\n",
      "(0.8, 'sample') dies out in a population of (0.6, 'map')\n",
      "(0.8, 'sample') dies out in a population of (0.7, 'map')\n",
      "(0.8, 'sample') dies out in a population of (0.8, 'map')\n",
      "(0.6, 'map') invades a population of (0.6, 'sample')\n",
      "(0.6, 'map') invades a population of (0.7, 'sample')\n",
      "(0.6, 'map') invades a population of (0.8, 'sample')\n",
      "(0.6, 'map') dies out in a population of (0.7, 'map')\n",
      "(0.6, 'map') invades a population of (0.8, 'map')\n",
      "(0.7, 'map') invades a population of (0.6, 'sample')\n",
      "(0.7, 'map') invades a population of (0.7, 'sample')\n",
      "(0.7, 'map') invades a population of (0.8, 'sample')\n",
      "(0.7, 'map') invades a population of (0.6, 'map')\n",
      "(0.7, 'map') invades a population of (0.8, 'map')\n",
      "(0.8, 'map') invades a population of (0.6, 'sample')\n",
      "(0.8, 'map') invades a population of (0.7, 'sample')\n",
      "(0.8, 'map') invades a population of (0.8, 'sample')\n",
      "(0.8, 'map') invades a population of (0.6, 'map')\n",
      "(0.8, 'map') dies out in a population of (0.7, 'map')\n"
     ]
    }
   ],
   "source": [
    "for minority in range(6):\n",
    "    for majority in range(6):\n",
    "        if results[minority][majority] > self_learning[majority]:\n",
    "            print(learners[minority], end=' ')\n",
    "            print('invades a population of', end=' ')\n",
    "            print(learners[majority])\n",
    "        elif results[minority][majority] < self_learning[majority]:\n",
    "            print(learners[minority], end=' ')\n",
    "            print('dies out in a population of', end=' ')        \n",
    "            print(learners[majority])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*So, it looks like MAP learners invade populations of samplers often, but never the other way around. Also, it looks like samplers that don't match the specific bias of the population die out, whereas that's not so clearly the case with MAP. However, there's a problem with this way of looking at things. This doesn't show us how big an advantage one type of learner has over another, and because these are simulation runs, the results are going to be quite variable and we might have a tiny difference showing up just by chance. Because of this, let's instead plot the results but using a ratio of mutant success to majority success. This will give us an estimate of the **selective advantage** the mutant has. We'll make a new table and ratios and plot this.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_results = []\n",
    "for minority in range(6):\n",
    "    new_row = []\n",
    "    for majority in range(6):\n",
    "        new_row.append(results[minority][majority] / self_learning[majority])\n",
    "    new_results.append(new_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.colorbar.Colorbar at 0x10631b8e0>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(1, 1)\n",
    "fig = ax.imshow(new_results, extent=[0,6,6,0], cmap='coolwarm')\n",
    "\n",
    "labels = ['.6 S', '.7 S', '.8 S', '.6 M', '.7 M', '.8 M']\n",
    "\n",
    "ax.set_xticks([.5,1.5,2.5,3.5,4.5,5.5])\n",
    "ax.set_xticklabels(labels)\n",
    "\n",
    "ax.set_yticks([.5,1.5,2.5,3.5,4.5,5.5])\n",
    "ax.set_yticklabels(labels)\n",
    "\n",
    "ax.set_ylabel(\"Mutant\")\n",
    "ax.set_xlabel(\"Majority\")\n",
    "\n",
    "plt.colorbar(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*So, from this plot, we can see clearly that MAP learners will invade sampling populations (the ratios in the bottom left are >1), but not vice versa (the ratios in the top right are <1). Also, there isn't a clear difference between different bias strengths. At least for MAP learners, the strong biases are no more likely to invade the weaker biases. This means that if there is some cost to maintaining a strong bias, or if mutations are likely to degrade rather than strengthen strong biases, we can expect to end up with MAP learners with weak biases. The important point is that weakly biased MAP learners have the same stationary distribution as strongly biased MAP learners (as we saw last week). That means that even for cases where we see strong universal patterns in language, we should not expect that to be supported by strong innate constraints. These kinds of constraints are not likely to evolve even if selection is favouring learners who can learn the language of the population they're born into!*"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "2254d95886abd510fdbe4740a73329c02480b002321aba449e3a8ac32ef99413"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
